{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tradução utilizando SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec - FastText\n",
    "\n",
    "- English: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
    "- Portuguese: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vec_file(file_name):\n",
    "    '''\n",
    "    Read file in .vec format\n",
    "    '''\n",
    "    return KeyedVectors.load_word2vec_format(\n",
    "        \"Fasttext/\" + file_name, \n",
    "        binary = False,\n",
    "        encoding ='unicode_escape',\n",
    "        unicode_errors = 'replace'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_MODEL = read_vec_file('cc.en.300.vec')\n",
    "PORTUGUESE_MODEL = read_vec_file('cc.pt.300.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon MASSIVE: https://github.com/alexa/massive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_amazon_corpus(corpus):\n",
    "    '''\n",
    "    Read file .jsonl from Amazon MASSIVE dataset\n",
    "    '''\n",
    "    return pd.read_json(\n",
    "        \"Amazon_Massive/\" + corpus, \n",
    "        lines = True\n",
    "    )['utt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_CORPUS = read_amazon_corpus('en-US.jsonl')\n",
    "PORTUGUESE_CORPUS = read_amazon_corpus('pt-PT.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tradução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pareamento dos dois idiomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_PT_DF = pd.merge(ENGLISH_CORPUS, PORTUGUESE_CORPUS, right_index = True, left_index = True)\n",
    "EN_PT_TUPLES = [tuple(x) for x in EN_PT_DF.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_LIST, PT_LIST = [], []\n",
    "for tuple_value in EN_PT_TUPLES:\n",
    "    EN_LIST.append(tuple_value[0].split(\" \"))\n",
    "    PT_LIST.append(tuple_value[1].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(language_list):\n",
    "    aux_list = []\n",
    "    for sentence in language_list:\n",
    "        for word in sentence:\n",
    "            aux_list.append(word)\n",
    "    \n",
    "    unique_list = []\n",
    "    for word in Counter(aux_list):\n",
    "        unique_list.append(word)\n",
    "\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_UNIQUE_WORDS = unique(EN_LIST)\n",
    "PT_UNIQUE_WORDS = unique(PT_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregamento de palavras para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH = []\n",
    "for w in EN_UNIQUE_WORDS:\n",
    "    try:\n",
    "        ENGLISH.append(ENGLISH_MODEL[w])\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "PORTUGUESE = []\n",
    "for w in PT_UNIQUE_WORDS:\n",
    "    try:\n",
    "        PORTUGUESE.append(PORTUGUESE_MODEL[w])\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo da matriz de tradução utilizando SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = min(len(ENGLISH), len(PORTUGUESE))\n",
    "U, Sig, Vt = np.linalg.svd(np.transpose(ENGLISH[:size]) @ PORTUGUESE[:size])\n",
    "translator = np.transpose(Vt) @ np.transpose(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Português -> Inglês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('excelled', 0.23170699179172516),\n",
       " ('scragged', 0.22841233015060425),\n",
       " ('amplifications', 0.2248193770647049),\n",
       " ('under-played', 0.2230806052684784),\n",
       " ('excels', 0.21914346516132355),\n",
       " ('Volcani', 0.21758601069450378),\n",
       " ('over-shadowed', 0.2172958105802536),\n",
       " ('InformationServing', 0.21379493176937103),\n",
       " ('Loathed', 0.2137456089258194),\n",
       " ('alliums', 0.2133820503950119)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENGLISH_MODEL.most_similar(translator @ PORTUGUESE_MODEL['sapato'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inglês -> Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deparamos', 0.3502688705921173),\n",
       " ('depara', 0.34006789326667786),\n",
       " ('thinwhitestripes', 0.33692777156829834),\n",
       " ('MOTORCICLISMOCAVALOSESPORTSDESPORTOS', 0.33510300517082214),\n",
       " ('tubarÃ£o', 0.33243897557258606),\n",
       " ('EspartilhosLegs', 0.33198824524879456),\n",
       " ('machuca', 0.32853350043296814),\n",
       " ('arranja', 0.32645148038864136),\n",
       " ('deparam', 0.3246495723724365),\n",
       " ('engatam', 0.3234668970108032)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PORTUGUESE_MODEL.most_similar(translator @ ENGLISH_MODEL['shoe'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
