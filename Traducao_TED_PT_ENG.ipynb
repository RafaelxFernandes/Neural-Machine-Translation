{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Traducao TED PT ENG",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo de tradução utilizando TED Talks\n",
        "Traduzindo de inglês para português brasileiro, e vice-versa."
      ],
      "metadata": {
        "id": "mKw2I57kIcOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bibliotecas"
      ],
      "metadata": {
        "id": "7QN3muszIxT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "nltk.download('punkt')\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78Ii2OB-JTXu",
        "outputId": "f54f6ca7-5c3b-4f51-f67f-1b3b7b67ad5f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqopyKbUBTGg",
        "outputId": "0274d891-d154-4d21-f004-5c3607914cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: translate-toolkit in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: lxml>=4.6.3 in /usr/local/lib/python3.7/dist-packages (from translate-toolkit) (4.8.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install sentencepiece\n",
        "!pip3 install transformers\n",
        "!pip3 install translate-toolkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from translate.storage.tmx import tmxfile"
      ],
      "metadata": {
        "id": "u8DRkbdxX0DS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Córpus\n",
        "Vamos utilizar um córpus de legendas de TED talks"
      ],
      "metadata": {
        "id": "12SFsRXEJIV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://object.pouta.csc.fi/OPUS-TED2020/v1/tmx/en-pt_br.tmx.gz\n",
        "!gunzip en-pt_br.tmx.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN01B86eI-hB",
        "outputId": "a87f6251-3cee-4c4e-fc32-80898ee23801"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-17 15:36:44--  https://object.pouta.csc.fi/OPUS-TED2020/v1/tmx/en-pt_br.tmx.gz\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32862474 (31M) [application/gzip]\n",
            "Saving to: ‘en-pt_br.tmx.gz’\n",
            "\n",
            "en-pt_br.tmx.gz     100%[===================>]  31.34M  12.7MB/s    in 2.5s    \n",
            "\n",
            "2022-03-17 15:36:47 (12.7 MB/s) - ‘en-pt_br.tmx.gz’ saved [32862474/32862474]\n",
            "\n",
            "gzip: en-pt_br.tmx already exists; do you wish to overwrite (y or n)? y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lendo o córpus e separando em conjuntos de treino e teste"
      ],
      "metadata": {
        "id": "o7LoeCUQJWtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ler córpus\n",
        "with open(\"en-pt_br.tmx\", 'rb') as fin:\n",
        "    f_en2pt_br = tmxfile(fin, 'en', 'pt')\n",
        "    f_pt_br2en = tmxfile(fin, 'pt', 'en')\n",
        "\n",
        "prefixo_en2pt_br = '>>pt_br<<'\n",
        "prefixo_pt_br2en = '>>en<<'\n",
        "\n",
        "# formatar as traduções corretamente \n",
        "data_en2pt_br = [\n",
        "                    { 'src': prefixo_en2pt_br + ' ' + w.source, 'trg': w.target } \n",
        "                    for w in f_en2pt_br.unit_iter()\n",
        "                ]\n",
        "\n",
        "data_pt_br2en = [\n",
        "                    { 'src': prefixo_pt_br2en + ' ' + w.target, 'trg': w.source } \n",
        "                    for w in f_pt_br2en.unit_iter()\n",
        "                ]\n",
        "\n",
        "# separar em conjuntos de treino e teste\n",
        "size_en2pt_br = int(len(data_en2pt_br) * 0.2)\n",
        "treino_en2pt_br = data_en2pt_br[size_en2pt_br:][:10000]\n",
        "teste_en2pt_br = data_en2pt_br[:size_en2pt_br][:1000]\n",
        "\n",
        "size_pt_br2en = int(len(data_pt_br2en) * 0.2)\n",
        "treino_pt_br2en = data_pt_br2en[size_pt_br2en:][:10000]\n",
        "teste_pt_br2en = data_pt_br2en[:size_pt_br2en][:1000]"
      ],
      "metadata": {
        "id": "jc28PjxBJOwx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treino_en2pt_br[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL5S2CTGJg6m",
        "outputId": "7cfcdb64-1e92-41cb-a923-99b6af85de25"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'src': \">>pt_br<< And as long as we've looked for explanations, we've wound up with something that gets closer and closer to science, which is hypotheses as to why we get sick, and as long as we've had hypotheses about why we get sick, we've tried to treat it as well. \",\n",
              " 'trg': 'E à medida que procuramos explicações, vamos chegando a conclusões que se aproximam cada vez mais da ciência, que é a hipótese sobre porque adoecemos, e à medida que temos hipóteses sobre porque adoecemos, também procuramos nos tratar. '}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "treino_pt_br2en[4234]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlkhbYLWevpO",
        "outputId": "41e5f2c3-42d0-4a6e-bcc9-396a2532a16d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'src': '>>en<< Essa é a verdade. ', 'trg': \"That's the truth. \"}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento\n",
        "Definindo parâmetros do modelo e treinamento"
      ],
      "metadata": {
        "id": "xEIDFHIjJwK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-5 \n",
        "epochs = 2\n",
        "batch_size = 16\n",
        "batch_status = 32\n",
        "early_stop = 5\n",
        "write_path_en2pt_br = 'model.pt'\n",
        "write_path_pt_br2en = 'model.en'"
      ],
      "metadata": {
        "id": "eZulD4cPJl_Q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separando dados em batches ( lotes )"
      ],
      "metadata": {
        "id": "aNKiTB-GKDdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_en2pt_br = DataLoader(treino_en2pt_br, batch_size = batch_size)\n",
        "dev_data_en2pt_br = DataLoader(teste_en2pt_br, batch_size = batch_size)\n",
        "\n",
        "train_data_pt_br2en = DataLoader(treino_pt_br2en, batch_size = batch_size)\n",
        "dev_data_pt_br2en = DataLoader(treino_pt_br2en, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "8Xz7yuBjKCuY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Método de avaliação"
      ],
      "metadata": {
        "id": "zXrOnb8hKgF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(tokenizer, model, dev_data, batch_size, batch_status, device):\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    y_real = []\n",
        "    y_pred = []\n",
        "    \n",
        "    for batch_idx, inp in enumerate(dev_data):\n",
        "        y_real.extend(inp['trg'])\n",
        "        \n",
        "        # tokenização\n",
        "        model_inputs = tokenizer(\n",
        "            inp['src'], \n",
        "            truncation = True, \n",
        "            padding = True, \n",
        "            max_length = 128, \n",
        "            return_tensors = \"pt\"\n",
        "        ).to(device)\n",
        "        \n",
        "        # tradução\n",
        "        generated_ids = model.generate(**model_inputs, num_beams = 1)\n",
        "        \n",
        "        # pós-processamento da tradução\n",
        "        output = tokenizer.batch_decode(generated_ids, skip_special_tokens = True)\n",
        "        y_pred.extend(output)\n",
        "    \n",
        "        # imprime resultados\n",
        "        if (batch_idx+1) % batch_status == 0:\n",
        "            print('Evaluation: [{}/{} ({:.0f}%)]'.format(batch_idx + 1, \\\n",
        "                len(dev_data), 100. * batch_idx / len(dev_data)))\n",
        "\n",
        "    # cálculo BLUE score\n",
        "    hyps, refs = [], []\n",
        "    \n",
        "    for i, snt_pred in enumerate(y_pred):\n",
        "        hyps.append(nltk.word_tokenize(snt_pred))\n",
        "        refs.append([nltk.word_tokenize(y_real[i])])\n",
        "    \n",
        "    bleu = corpus_bleu(refs, hyps)\n",
        "\n",
        "    return bleu"
      ],
      "metadata": {
        "id": "-l_Cd8L1KfVC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Método do treinamento"
      ],
      "metadata": {
        "id": "tH2wZ22zLFHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(tokenizer, model, train_data, dev_data, optimizer, num_epochs, \n",
        "          batch_size, batch_status, device, write_path, early_stop = 5):\n",
        "    \n",
        "    max_bleu = evaluate(tokenizer, model, dev_data, batch_size, batch_status, device)\n",
        "    print('BLEU inicial:', max_bleu)\n",
        "    \n",
        "    model.train()\n",
        "    repeat = 0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        losses = []\n",
        "        batch_src, batch_trg = [], []\n",
        "\n",
        "        for batch_idx, inp in enumerate(train_data):\n",
        "            # inicializa zerando o gradiente\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # tokenização\n",
        "            model_inputs = tokenizer(\n",
        "                inp['src'], \n",
        "                truncation = True,\n",
        "                padding = True, \n",
        "                max_length = 128, \n",
        "                return_tensors = \"pt\"\n",
        "            ).to(device)\n",
        "            \n",
        "            with tokenizer.as_target_tokenizer():\n",
        "                labels = tokenizer(\n",
        "                    inp['trg'], \n",
        "                    truncation = True, \n",
        "                    padding = True, \n",
        "                    max_length = 128, \n",
        "                    return_tensors = \"pt\"\n",
        "                ).input_ids.to(device)\n",
        "            \n",
        "            # tradução\n",
        "            output = model(**model_inputs, labels=labels) # forward pass\n",
        "\n",
        "            # cálculo perda\n",
        "            loss = output.loss\n",
        "            losses.append(float(loss))\n",
        "\n",
        "            # backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_src, batch_trg = [], []\n",
        "\n",
        "            # imprime resultados\n",
        "            if (batch_idx+1) % batch_status == 0:\n",
        "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTotal Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx+1, len(train_data), 100. * batch_idx / len(train_data), \n",
        "                float(loss), round(sum(losses) / len(losses), 5)))\n",
        "\n",
        "        bleu = evaluate(tokenizer, model, dev_data, batch_size, batch_status, device)\n",
        "        print('BLEU:', bleu)\n",
        "        \n",
        "        if bleu > max_bleu:\n",
        "            max_bleu = bleu\n",
        "            repeat = 0\n",
        "\n",
        "            print('Saving best model...')\n",
        "            torch.save(model, write_path)\n",
        "        else:\n",
        "            repeat += 1\n",
        "\n",
        "        if repeat == early_stop:\n",
        "            break"
      ],
      "metadata": {
        "id": "ntr4XkyFLEJa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicializando o modelo"
      ],
      "metadata": {
        "id": "5xjqXQTnL8Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-ROMANCE\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ROMANCE\")\n",
        "optimizer = optim.AdamW(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "0bwoZUFzL1hb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinando inglês -> português brasileiro"
      ],
      "metadata": {
        "id": "hU8U5fT8MCOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    tokenizer, model, train_data_en2pt_br, dev_data_en2pt_br, optimizer, epochs, \n",
        "    batch_size, batch_status, device, write_path_en2pt_br, early_stop\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TSBLPyTMCyI",
        "outputId": "b32a5b56-f776-449b-dd38-de5c3c3350d3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation: [32/63 (49%)]\n",
            "BLEU inicial: 0.41589108912086625\n",
            "Train Epoch: 0 [32/625 (5%)]\tLoss: 0.571615\tTotal Loss: 0.919390\n",
            "Train Epoch: 0 [64/625 (10%)]\tLoss: 0.463009\tTotal Loss: 0.753600\n",
            "Train Epoch: 0 [96/625 (15%)]\tLoss: 0.820054\tTotal Loss: 0.680270\n",
            "Train Epoch: 0 [128/625 (20%)]\tLoss: 0.449106\tTotal Loss: 0.623230\n",
            "Train Epoch: 0 [160/625 (25%)]\tLoss: 0.503968\tTotal Loss: 0.598760\n",
            "Train Epoch: 0 [192/625 (31%)]\tLoss: 0.321125\tTotal Loss: 0.568030\n",
            "Train Epoch: 0 [224/625 (36%)]\tLoss: 0.433067\tTotal Loss: 0.554170\n",
            "Train Epoch: 0 [256/625 (41%)]\tLoss: 0.392383\tTotal Loss: 0.533040\n",
            "Train Epoch: 0 [288/625 (46%)]\tLoss: 0.893320\tTotal Loss: 0.539610\n",
            "Train Epoch: 0 [320/625 (51%)]\tLoss: 0.355073\tTotal Loss: 0.525290\n",
            "Train Epoch: 0 [352/625 (56%)]\tLoss: 0.444844\tTotal Loss: 0.515850\n",
            "Train Epoch: 0 [384/625 (61%)]\tLoss: 0.696781\tTotal Loss: 0.515320\n",
            "Train Epoch: 0 [416/625 (66%)]\tLoss: 0.392406\tTotal Loss: 0.505580\n",
            "Train Epoch: 0 [448/625 (72%)]\tLoss: 0.529203\tTotal Loss: 0.507200\n",
            "Train Epoch: 0 [480/625 (77%)]\tLoss: 0.426089\tTotal Loss: 0.504180\n",
            "Train Epoch: 0 [512/625 (82%)]\tLoss: 0.328323\tTotal Loss: 0.499000\n",
            "Train Epoch: 0 [544/625 (87%)]\tLoss: 0.696134\tTotal Loss: 0.498850\n",
            "Train Epoch: 0 [576/625 (92%)]\tLoss: 0.255454\tTotal Loss: 0.499520\n",
            "Train Epoch: 0 [608/625 (97%)]\tLoss: 0.451970\tTotal Loss: 0.495840\n",
            "Evaluation: [32/63 (49%)]\n",
            "BLEU: 0.42664380376062433\n",
            "Saving best model...\n",
            "Train Epoch: 1 [32/625 (5%)]\tLoss: 0.421327\tTotal Loss: 0.394750\n",
            "Train Epoch: 1 [64/625 (10%)]\tLoss: 0.373669\tTotal Loss: 0.431090\n",
            "Train Epoch: 1 [96/625 (15%)]\tLoss: 0.701362\tTotal Loss: 0.431950\n",
            "Train Epoch: 1 [128/625 (20%)]\tLoss: 0.371709\tTotal Loss: 0.417490\n",
            "Train Epoch: 1 [160/625 (25%)]\tLoss: 0.426823\tTotal Loss: 0.415780\n",
            "Train Epoch: 1 [192/625 (31%)]\tLoss: 0.265442\tTotal Loss: 0.403690\n",
            "Train Epoch: 1 [224/625 (36%)]\tLoss: 0.366336\tTotal Loss: 0.401520\n",
            "Train Epoch: 1 [256/625 (41%)]\tLoss: 0.308211\tTotal Loss: 0.391170\n",
            "Train Epoch: 1 [288/625 (46%)]\tLoss: 0.779309\tTotal Loss: 0.404590\n",
            "Train Epoch: 1 [320/625 (51%)]\tLoss: 0.294913\tTotal Loss: 0.397210\n",
            "Train Epoch: 1 [352/625 (56%)]\tLoss: 0.393741\tTotal Loss: 0.393180\n",
            "Train Epoch: 1 [384/625 (61%)]\tLoss: 0.605483\tTotal Loss: 0.396220\n",
            "Train Epoch: 1 [416/625 (66%)]\tLoss: 0.318424\tTotal Loss: 0.390840\n",
            "Train Epoch: 1 [448/625 (72%)]\tLoss: 0.447165\tTotal Loss: 0.394170\n",
            "Train Epoch: 1 [480/625 (77%)]\tLoss: 0.355519\tTotal Loss: 0.393200\n",
            "Train Epoch: 1 [512/625 (82%)]\tLoss: 0.276100\tTotal Loss: 0.390870\n",
            "Train Epoch: 1 [544/625 (87%)]\tLoss: 0.588988\tTotal Loss: 0.392480\n",
            "Train Epoch: 1 [576/625 (92%)]\tLoss: 0.214814\tTotal Loss: 0.394960\n",
            "Train Epoch: 1 [608/625 (97%)]\tLoss: 0.372304\tTotal Loss: 0.393040\n",
            "Evaluation: [32/63 (49%)]\n",
            "BLEU: 0.42261372887187404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinando português brasileiro -> inglês"
      ],
      "metadata": {
        "id": "uPna-r8qfdrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    tokenizer, model, train_data_pt_br2en, dev_data_pt_br2en, optimizer, epochs, \n",
        "    batch_size, batch_status, device, write_path_pt_br2en, early_stop\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zphyZ6affgTk",
        "outputId": "0976f02a-6700-4054-d2cb-771b228d7f17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation: [32/625 (5%)]\n",
            "Evaluation: [64/625 (10%)]\n",
            "Evaluation: [96/625 (15%)]\n",
            "Evaluation: [128/625 (20%)]\n",
            "Evaluation: [160/625 (25%)]\n",
            "Evaluation: [192/625 (31%)]\n",
            "Evaluation: [224/625 (36%)]\n",
            "Evaluation: [256/625 (41%)]\n",
            "Evaluation: [288/625 (46%)]\n",
            "Evaluation: [320/625 (51%)]\n",
            "Evaluation: [352/625 (56%)]\n",
            "Evaluation: [384/625 (61%)]\n",
            "Evaluation: [416/625 (66%)]\n",
            "Evaluation: [448/625 (72%)]\n",
            "Evaluation: [480/625 (77%)]\n",
            "Evaluation: [512/625 (82%)]\n",
            "Evaluation: [544/625 (87%)]\n",
            "Evaluation: [576/625 (92%)]\n",
            "Evaluation: [608/625 (97%)]\n",
            "BLEU inicial: 0.01053717427217039\n",
            "Train Epoch: 0 [32/625 (5%)]\tLoss: 1.290827\tTotal Loss: 1.680000\n",
            "Train Epoch: 0 [64/625 (10%)]\tLoss: 1.840085\tTotal Loss: 1.680880\n",
            "Train Epoch: 0 [96/625 (15%)]\tLoss: 1.909701\tTotal Loss: 1.639970\n",
            "Train Epoch: 0 [128/625 (20%)]\tLoss: 1.094319\tTotal Loss: 1.583320\n",
            "Train Epoch: 0 [160/625 (25%)]\tLoss: 1.285535\tTotal Loss: 1.537460\n",
            "Train Epoch: 0 [192/625 (31%)]\tLoss: 1.091980\tTotal Loss: 1.498820\n",
            "Train Epoch: 0 [224/625 (36%)]\tLoss: 1.024149\tTotal Loss: 1.457720\n",
            "Train Epoch: 0 [256/625 (41%)]\tLoss: 1.155624\tTotal Loss: 1.425830\n",
            "Train Epoch: 0 [288/625 (46%)]\tLoss: 1.214940\tTotal Loss: 1.419490\n",
            "Train Epoch: 0 [320/625 (51%)]\tLoss: 1.191905\tTotal Loss: 1.397900\n",
            "Train Epoch: 0 [352/625 (56%)]\tLoss: 1.328228\tTotal Loss: 1.372390\n",
            "Train Epoch: 0 [384/625 (61%)]\tLoss: 1.552923\tTotal Loss: 1.365240\n",
            "Train Epoch: 0 [416/625 (66%)]\tLoss: 0.983621\tTotal Loss: 1.340300\n",
            "Train Epoch: 0 [448/625 (72%)]\tLoss: 1.156943\tTotal Loss: 1.335880\n",
            "Train Epoch: 0 [480/625 (77%)]\tLoss: 0.892285\tTotal Loss: 1.322240\n",
            "Train Epoch: 0 [512/625 (82%)]\tLoss: 0.898863\tTotal Loss: 1.305970\n",
            "Train Epoch: 0 [544/625 (87%)]\tLoss: 1.524516\tTotal Loss: 1.291460\n",
            "Train Epoch: 0 [576/625 (92%)]\tLoss: 0.946910\tTotal Loss: 1.281410\n",
            "Train Epoch: 0 [608/625 (97%)]\tLoss: 0.979680\tTotal Loss: 1.268150\n",
            "Evaluation: [32/625 (5%)]\n",
            "Evaluation: [64/625 (10%)]\n",
            "Evaluation: [96/625 (15%)]\n",
            "Evaluation: [128/625 (20%)]\n",
            "Evaluation: [160/625 (25%)]\n",
            "Evaluation: [192/625 (31%)]\n",
            "Evaluation: [224/625 (36%)]\n",
            "Evaluation: [256/625 (41%)]\n",
            "Evaluation: [288/625 (46%)]\n",
            "Evaluation: [320/625 (51%)]\n",
            "Evaluation: [352/625 (56%)]\n",
            "Evaluation: [384/625 (61%)]\n",
            "Evaluation: [416/625 (66%)]\n",
            "Evaluation: [448/625 (72%)]\n",
            "Evaluation: [480/625 (77%)]\n",
            "Evaluation: [512/625 (82%)]\n",
            "Evaluation: [544/625 (87%)]\n",
            "Evaluation: [576/625 (92%)]\n",
            "Evaluation: [608/625 (97%)]\n",
            "BLEU: 0.13943036249601426\n",
            "Saving best model...\n",
            "Train Epoch: 1 [32/625 (5%)]\tLoss: 0.765870\tTotal Loss: 0.847500\n",
            "Train Epoch: 1 [64/625 (10%)]\tLoss: 1.204434\tTotal Loss: 0.933800\n",
            "Train Epoch: 1 [96/625 (15%)]\tLoss: 1.251346\tTotal Loss: 0.952050\n",
            "Train Epoch: 1 [128/625 (20%)]\tLoss: 0.727055\tTotal Loss: 0.945610\n",
            "Train Epoch: 1 [160/625 (25%)]\tLoss: 0.913222\tTotal Loss: 0.938470\n",
            "Train Epoch: 1 [192/625 (31%)]\tLoss: 0.651069\tTotal Loss: 0.924780\n",
            "Train Epoch: 1 [224/625 (36%)]\tLoss: 0.737737\tTotal Loss: 0.908500\n",
            "Train Epoch: 1 [256/625 (41%)]\tLoss: 0.753919\tTotal Loss: 0.897850\n",
            "Train Epoch: 1 [288/625 (46%)]\tLoss: 0.909464\tTotal Loss: 0.908330\n",
            "Train Epoch: 1 [320/625 (51%)]\tLoss: 0.877520\tTotal Loss: 0.900900\n",
            "Train Epoch: 1 [352/625 (56%)]\tLoss: 0.840256\tTotal Loss: 0.890470\n",
            "Train Epoch: 1 [384/625 (61%)]\tLoss: 1.232751\tTotal Loss: 0.893660\n",
            "Train Epoch: 1 [416/625 (66%)]\tLoss: 0.690663\tTotal Loss: 0.882860\n",
            "Train Epoch: 1 [448/625 (72%)]\tLoss: 0.823420\tTotal Loss: 0.887390\n",
            "Train Epoch: 1 [480/625 (77%)]\tLoss: 0.610920\tTotal Loss: 0.882370\n",
            "Train Epoch: 1 [512/625 (82%)]\tLoss: 0.648396\tTotal Loss: 0.875270\n",
            "Train Epoch: 1 [544/625 (87%)]\tLoss: 1.213713\tTotal Loss: 0.870280\n",
            "Train Epoch: 1 [576/625 (92%)]\tLoss: 0.717510\tTotal Loss: 0.869160\n",
            "Train Epoch: 1 [608/625 (97%)]\tLoss: 0.751287\tTotal Loss: 0.863010\n",
            "Evaluation: [32/625 (5%)]\n",
            "Evaluation: [64/625 (10%)]\n",
            "Evaluation: [96/625 (15%)]\n",
            "Evaluation: [128/625 (20%)]\n",
            "Evaluation: [160/625 (25%)]\n",
            "Evaluation: [192/625 (31%)]\n",
            "Evaluation: [224/625 (36%)]\n",
            "Evaluation: [256/625 (41%)]\n",
            "Evaluation: [288/625 (46%)]\n",
            "Evaluation: [320/625 (51%)]\n",
            "Evaluation: [352/625 (56%)]\n",
            "Evaluation: [384/625 (61%)]\n",
            "Evaluation: [416/625 (66%)]\n",
            "Evaluation: [448/625 (72%)]\n",
            "Evaluation: [480/625 (77%)]\n",
            "Evaluation: [512/625 (82%)]\n",
            "Evaluation: [544/625 (87%)]\n",
            "Evaluation: [576/625 (92%)]\n",
            "Evaluation: [608/625 (97%)]\n",
            "BLEU: 0.19004685805189034\n",
            "Saving best model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados"
      ],
      "metadata": {
        "id": "L0LLDucgM0lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inglês -> Português brasileiro"
      ],
      "metadata": {
        "id": "WQ5mhZVcgBv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sentenças a serem traduzidas\n",
        "batch_input_str = (\n",
        "    (\">>pt_br<< Please, don't fail me now.\"), \n",
        "    (\">>pt_br<< Who is a good translator? You are!\"), \n",
        "    (\">>pt_br<< I hope you are able to translate a big sentence, because people nowadays love texting. And I want to present this to my teacher and colleagues, so you have to work!\"),\n",
        "    (\">>pt_br<< I really don't want to study tonight but I have to do it because I want to graduate and get a job and have a lot of money.\")\n",
        ")\n",
        "\n",
        "# tokenizando as sentenças\n",
        "encoded = tokenizer(batch_input_str, return_tensors = 'pt', padding = True).to(device)\n",
        "\n",
        "# traduzindo\n",
        "translated = model.generate(**encoded)\n",
        "\n",
        "# preparando a saída\n",
        "tokenizer.batch_decode(translated, skip_special_tokens = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35zFc-xxMUSp",
        "outputId": "38f56fda-3c2a-4129-cbc7-bd70cd8d34f7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Por favor, não falle comigo agora.',\n",
              " 'Quem é um bom tradutor?',\n",
              " 'Espero que você seja capaz de traduzir uma frase grande, porque as pessoas hoje adoram SMS. E eu quero apresentar isso ao meu professor e colegas, então você tem que trabalhar!',\n",
              " 'Eu realmente não quero estudar hoje, mas eu tenho que fazer isso porque eu quero me formar, arrumar um emprego e ter muito dinheiro.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Português brasileiro -> Inglês"
      ],
      "metadata": {
        "id": "0-nbA-GTgCPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_input_str = (\n",
        "    (\">>en<< Será que isso vai funcionar?\"),\n",
        "    (\">>en<< Teste número 2. Você consegue traduzir isso que eu sei!\"),\n",
        "    (\">>en<< Acho que eu preciso deixar você rodando por mais tempo, né?\"),\n",
        "    (\">>en<< Eu sei que eu deveria ser mais criativo nos meus testes, mas não acredito que consegui traduzir de português brasileiro para inglês, mesmo com um BLEU tão baixo.\")\n",
        ")\n",
        "\n",
        "# tokenizando as sentenças\n",
        "encoded = tokenizer(batch_input_str, return_tensors = 'pt', padding = True).to(device)\n",
        "\n",
        "# traduzindo\n",
        "translated = model.generate(**encoded)\n",
        "\n",
        "# preparando a saída\n",
        "tokenizer.batch_decode(translated, skip_special_tokens = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x61pMDuIgJ8g",
        "outputId": "43e81d65-ade9-4e2d-f3ae-2abe32561da0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I will see what this will work?',\n",
              " \"It's going to trade this that I know!\",\n",
              " 'I think I need to learn you walking for most time, yes?',\n",
              " \"I know that I should be more creative in my trials, but I'm not believe that I can't trade brain to English brain translate with a BLEU's down.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}