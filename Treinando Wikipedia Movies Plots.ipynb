{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando Movie Plos from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Origin/Ethnicity</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Wiki Page</th>\n",
       "      <th>Plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1901</td>\n",
       "      <td>Kansas Saloon Smashers</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...</td>\n",
       "      <td>A bartender is working at a saloon, serving dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1901</td>\n",
       "      <td>Love by the Light of the Moon</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Love_by_the_Ligh...</td>\n",
       "      <td>The moon, painted with a smiling face hangs ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1901</td>\n",
       "      <td>The Martyred Presidents</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Martyred_Pre...</td>\n",
       "      <td>The film, just over a minute long, is composed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1901</td>\n",
       "      <td>Terrible Teddy, the Grizzly King</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Terrible_Teddy,_...</td>\n",
       "      <td>Lasting just 61 seconds and consisting of two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1902</td>\n",
       "      <td>Jack and the Beanstalk</td>\n",
       "      <td>American</td>\n",
       "      <td>George S. Fleming, Edwin S. Porter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jack_and_the_Bea...</td>\n",
       "      <td>The earliest known adaptation of the classic f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Release Year                             Title Origin/Ethnicity  \\\n",
       "0          1901            Kansas Saloon Smashers         American   \n",
       "1          1901     Love by the Light of the Moon         American   \n",
       "2          1901           The Martyred Presidents         American   \n",
       "3          1901  Terrible Teddy, the Grizzly King         American   \n",
       "4          1902            Jack and the Beanstalk         American   \n",
       "\n",
       "                             Director Cast    Genre  \\\n",
       "0                             Unknown  NaN  unknown   \n",
       "1                             Unknown  NaN  unknown   \n",
       "2                             Unknown  NaN  unknown   \n",
       "3                             Unknown  NaN  unknown   \n",
       "4  George S. Fleming, Edwin S. Porter  NaN  unknown   \n",
       "\n",
       "                                           Wiki Page  \\\n",
       "0  https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...   \n",
       "1  https://en.wikipedia.org/wiki/Love_by_the_Ligh...   \n",
       "2  https://en.wikipedia.org/wiki/The_Martyred_Pre...   \n",
       "3  https://en.wikipedia.org/wiki/Terrible_Teddy,_...   \n",
       "4  https://en.wikipedia.org/wiki/Jack_and_the_Bea...   \n",
       "\n",
       "                                                Plot  \n",
       "0  A bartender is working at a saloon, serving dr...  \n",
       "1  The moon, painted with a smiling face hangs ov...  \n",
       "2  The film, just over a minute long, is composed...  \n",
       "3  Lasting just 61 seconds and consisting of two ...  \n",
       "4  The earliest known adaptation of the classic f...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Dados/wiki_movie_plots_deduped.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    A bartender is working at a saloon, serving dr...\n",
       "1    The moon, painted with a smiling face hangs ov...\n",
       "2    The film, just over a minute long, is composed...\n",
       "3    Lasting just 61 seconds and consisting of two ...\n",
       "4    The earliest known adaptation of the classic f...\n",
       "Name: Plot, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df[\"Plot\"]\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de stopwords e tokenização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'bartender',\n",
       " 'is',\n",
       " 'working',\n",
       " 'at',\n",
       " 'a',\n",
       " 'saloon',\n",
       " ',',\n",
       " 'serving',\n",
       " 'drinks',\n",
       " 'to',\n",
       " 'customers',\n",
       " '.',\n",
       " 'after',\n",
       " 'he',\n",
       " 'fills',\n",
       " 'a',\n",
       " 'stereotypically',\n",
       " 'irish',\n",
       " 'man',\n",
       " \"'s\",\n",
       " 'bucket',\n",
       " 'with',\n",
       " 'beer',\n",
       " ',',\n",
       " 'carrie',\n",
       " 'nation',\n",
       " 'and',\n",
       " 'her',\n",
       " 'followers',\n",
       " 'burst',\n",
       " 'inside',\n",
       " '.',\n",
       " 'they',\n",
       " 'assault',\n",
       " 'the',\n",
       " 'irish',\n",
       " 'man',\n",
       " ',',\n",
       " 'pulling',\n",
       " 'his',\n",
       " 'hat',\n",
       " 'over',\n",
       " 'his',\n",
       " 'eyes',\n",
       " 'and',\n",
       " 'then',\n",
       " 'dumping',\n",
       " 'the',\n",
       " 'beer',\n",
       " 'over',\n",
       " 'his',\n",
       " 'head',\n",
       " '.',\n",
       " 'the',\n",
       " 'group',\n",
       " 'then',\n",
       " 'begin',\n",
       " 'wrecking',\n",
       " 'the',\n",
       " 'bar',\n",
       " ',',\n",
       " 'smashing',\n",
       " 'the',\n",
       " 'fixtures',\n",
       " ',',\n",
       " 'mirrors',\n",
       " ',',\n",
       " 'and',\n",
       " 'breaking',\n",
       " 'the',\n",
       " 'cash',\n",
       " 'register',\n",
       " '.',\n",
       " 'the',\n",
       " 'bartender',\n",
       " 'then',\n",
       " 'sprays',\n",
       " 'seltzer',\n",
       " 'water',\n",
       " 'in',\n",
       " 'nation',\n",
       " \"'s\",\n",
       " 'face',\n",
       " 'before',\n",
       " 'a',\n",
       " 'group',\n",
       " 'of',\n",
       " 'policemen',\n",
       " 'appear',\n",
       " 'and',\n",
       " 'order',\n",
       " 'everybody',\n",
       " 'to',\n",
       " 'leave',\n",
       " '.',\n",
       " '[',\n",
       " '1',\n",
       " ']']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = list(corpus)\n",
    "\n",
    "# plots = [nltk.word_tokenize(word.lower()) for word in corpus if not word.lower() in stopwords]\n",
    "plots = [nltk.word_tokenize(word.lower()) for word in corpus[:5] if not word.lower() in stopwords]\n",
    "plots[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tamanho de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = int(len(plots) * 0.2)\n",
    "treino = plots[size:]\n",
    "teste = plots[:size]\n",
    "\n",
    "len(treino), len(teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW - Continuos Bag-of-words\n",
    "Rede neural proposta para o treinamento de word embeddings livre de contexto. O modelo CBOW aprende representações vetoriais ao ser treinado na tarefa de prever um determinado token-alvo dado os tokens ao seu redor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar definindo o vocabulário para o qual treinaremos representações vetoriais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(texts):\n",
    "\n",
    "    vocab = []\n",
    "\n",
    "    for row in texts:\n",
    "        vocab.extend(row)\n",
    "\n",
    "    vocab = list(set(vocab))\n",
    "    vocab += ['<pad>', '<oov>'] # padding & out of vocabulary\n",
    "\n",
    "    w2id = { w:i for i, w in enumerate(vocab) } # word -> id\n",
    "    id2w = { i:w for i, w in enumerate(vocab) } # id -> word\n",
    "\n",
    "    return vocab, w2id, id2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palavras:  247\n"
     ]
    }
   ],
   "source": [
    "vocab, w2id, id2w = get_vocab(treino)\n",
    "print('Número de palavras: ', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir então os exemplos de treinamento. Como mencionado acima, as redes neurais CBOW aprendem os word embeddings ao serem treinadas na tarefa de prever um token dado um contexto de tokens ao redor. Portanto, devemos construir instâncias compostas de um token e seus tokens vizinhos de acordo com um tamanho de janela ( window size ). Neste caso, vamos definir uma janela de tamanho 2, isto é, vamos construir um contexto compostos pelos 2 tokens anteriores e os 2 tokens posteriores de um token alvo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_window(tokens, size = 3):\n",
    "\n",
    "    tokens = (['<pad>'] * size) + tokens + (['<pad>'] * size)\n",
    "\n",
    "    contexts = []\n",
    "\n",
    "    for i in range(size, len(tokens) - size):\n",
    "        context = tokens[i-size : i] + tokens[i+1 : i+size+1]\n",
    "        word = tokens[i]\n",
    "        contexts.append({ 'context': ' '.join(context), 'word': word })\n",
    "\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "window_size = 3\n",
    "\n",
    "for plot in treino:\n",
    "    data.extend(context_window(plot, size = window_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'face hangs over park at night', 'word': 'a'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': '<pad> <pad> the , just over', 'word': 'film'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir o modelo utilizando Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, inp_dim, nvocab, window_size, w2id, device):\n",
    "        '''\n",
    "        Inicializando uma rede neural CBOW\n",
    "\n",
    "        Parâmetros:\n",
    "        - inp_dim: dimensão dos embeddings\n",
    "        - nvocab: tamanho do vocabulário de palavras para as quais treinaremos word embeddings\n",
    "        - window_size: tamanho da janela de contexto\n",
    "        - w2id: mapping de um token para seu índice na matriz de embeddings\n",
    "        - device: dispositivo onde a rede neural será alocada (cpu ou cuda)\n",
    "        '''\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.w2id = w2id\n",
    "        self.lookup = nn.Embedding(nvocab, inp_dim)\n",
    "        self.weight = nn.Linear(2 * window_size * inp_dim, nvocab)\n",
    "        self.softmax = nn.LogSoftmax(1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        Forward pass\n",
    "\n",
    "        Parâmetros:\n",
    "        - X: lista de contextos de entrada\n",
    "\n",
    "        Return:\n",
    "        Probabilidade entre as palavras do vocabulário\n",
    "        '''\n",
    "        contexts = []\n",
    "\n",
    "        for context in X:\n",
    "            indexes = []\n",
    "\n",
    "            for token in context.split():\n",
    "                try:\n",
    "                    indexes.append(w2id[token])\n",
    "                except:\n",
    "                    indexes.append(w2id['<oov>']) # out of vocabulary\n",
    "            \n",
    "            contexts.append(indexes)\n",
    "\n",
    "        contexts = torch.tensor(contexts).to(self.device)\n",
    "        embeddings = self.lookup(contexts)\n",
    "\n",
    "        batch_size, window_size, inp_dim = embeddings.size()\n",
    "        concatenation = embeddings.view(batch_size, window_size * inp_dim)\n",
    "\n",
    "        z = self.weight(concatenation)\n",
    "\n",
    "        return self.softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo os parâmetros da rede neural e de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inp_dim = 300\n",
    "nvocab = len(vocab)\n",
    "nepochs = 1\n",
    "batch_size = 1 # 256\n",
    "batch_status = 1 # 256\n",
    "learning_rate = 0.01\n",
    "window_size = window_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializando o modelo, a função de erro e o otimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(\n",
    "    inp_dim,\n",
    "    nvocab,\n",
    "    window_size = window_size,\n",
    "    w2id = w2id,\n",
    "    device = device\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando o conjunto de treinamento em lotes ( batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1/526 (0%)]\tLoss: 5.929131\tTotal Loss: 5.929130\n",
      "Train Epoch: 1 [2/526 (0%)]\tLoss: 5.882979\tTotal Loss: 5.906060\n",
      "Train Epoch: 1 [3/526 (0%)]\tLoss: 5.433474\tTotal Loss: 5.748530\n",
      "Train Epoch: 1 [4/526 (1%)]\tLoss: 5.857660\tTotal Loss: 5.775810\n",
      "Train Epoch: 1 [5/526 (1%)]\tLoss: 6.126052\tTotal Loss: 5.845860\n",
      "Train Epoch: 1 [6/526 (1%)]\tLoss: 5.535096\tTotal Loss: 5.794070\n",
      "Train Epoch: 1 [7/526 (1%)]\tLoss: 8.251540\tTotal Loss: 6.145130\n",
      "Train Epoch: 1 [8/526 (1%)]\tLoss: 6.521366\tTotal Loss: 6.192160\n",
      "Train Epoch: 1 [9/526 (2%)]\tLoss: 5.270750\tTotal Loss: 6.089780\n",
      "Train Epoch: 1 [10/526 (2%)]\tLoss: 9.075246\tTotal Loss: 6.388330\n",
      "Train Epoch: 1 [11/526 (2%)]\tLoss: 5.198109\tTotal Loss: 6.280130\n",
      "Train Epoch: 1 [12/526 (2%)]\tLoss: 6.628568\tTotal Loss: 6.309160\n",
      "Train Epoch: 1 [13/526 (2%)]\tLoss: 5.961961\tTotal Loss: 6.282460\n",
      "Train Epoch: 1 [14/526 (2%)]\tLoss: 7.782248\tTotal Loss: 6.389580\n",
      "Train Epoch: 1 [15/526 (3%)]\tLoss: 22.305626\tTotal Loss: 7.450650\n",
      "Train Epoch: 1 [16/526 (3%)]\tLoss: 5.838254\tTotal Loss: 7.349880\n",
      "Train Epoch: 1 [17/526 (3%)]\tLoss: 11.402963\tTotal Loss: 7.588300\n",
      "Train Epoch: 1 [18/526 (3%)]\tLoss: 6.631768\tTotal Loss: 7.535160\n",
      "Train Epoch: 1 [19/526 (3%)]\tLoss: 6.311583\tTotal Loss: 7.470760\n",
      "Train Epoch: 1 [20/526 (4%)]\tLoss: 6.998258\tTotal Loss: 7.447130\n",
      "Train Epoch: 1 [21/526 (4%)]\tLoss: 5.094269\tTotal Loss: 7.335090\n",
      "Train Epoch: 1 [22/526 (4%)]\tLoss: 15.367412\tTotal Loss: 7.700200\n",
      "Train Epoch: 1 [23/526 (4%)]\tLoss: 8.379089\tTotal Loss: 7.729710\n",
      "Train Epoch: 1 [24/526 (4%)]\tLoss: 14.908791\tTotal Loss: 8.028840\n",
      "Train Epoch: 1 [25/526 (5%)]\tLoss: 8.774786\tTotal Loss: 8.058680\n",
      "Train Epoch: 1 [26/526 (5%)]\tLoss: 14.480480\tTotal Loss: 8.305670\n",
      "Train Epoch: 1 [27/526 (5%)]\tLoss: 9.075740\tTotal Loss: 8.334190\n",
      "Train Epoch: 1 [28/526 (5%)]\tLoss: 15.861497\tTotal Loss: 8.603020\n",
      "Train Epoch: 1 [29/526 (5%)]\tLoss: 5.925007\tTotal Loss: 8.510680\n",
      "Train Epoch: 1 [30/526 (6%)]\tLoss: 17.118795\tTotal Loss: 8.797620\n",
      "Train Epoch: 1 [31/526 (6%)]\tLoss: 12.939047\tTotal Loss: 8.931210\n",
      "Train Epoch: 1 [32/526 (6%)]\tLoss: 2.284788\tTotal Loss: 8.723510\n",
      "Train Epoch: 1 [33/526 (6%)]\tLoss: 7.155085\tTotal Loss: 8.675980\n",
      "Train Epoch: 1 [34/526 (6%)]\tLoss: 12.930099\tTotal Loss: 8.801100\n",
      "Train Epoch: 1 [35/526 (6%)]\tLoss: 17.608351\tTotal Loss: 9.052740\n",
      "Train Epoch: 1 [36/526 (7%)]\tLoss: 26.359253\tTotal Loss: 9.533480\n",
      "Train Epoch: 1 [37/526 (7%)]\tLoss: 17.005535\tTotal Loss: 9.735420\n",
      "Train Epoch: 1 [38/526 (7%)]\tLoss: 15.492293\tTotal Loss: 9.886920\n",
      "Train Epoch: 1 [39/526 (7%)]\tLoss: 18.441978\tTotal Loss: 10.106280\n",
      "Train Epoch: 1 [40/526 (7%)]\tLoss: 16.833776\tTotal Loss: 10.274470\n",
      "Train Epoch: 1 [41/526 (8%)]\tLoss: 2.061809\tTotal Loss: 10.074160\n",
      "Train Epoch: 1 [42/526 (8%)]\tLoss: 15.510194\tTotal Loss: 10.203590\n",
      "Train Epoch: 1 [43/526 (8%)]\tLoss: 19.306274\tTotal Loss: 10.415280\n",
      "Train Epoch: 1 [44/526 (8%)]\tLoss: 18.849054\tTotal Loss: 10.606960\n",
      "Train Epoch: 1 [45/526 (8%)]\tLoss: 13.773268\tTotal Loss: 10.677320\n",
      "Train Epoch: 1 [46/526 (9%)]\tLoss: 0.311827\tTotal Loss: 10.451980\n",
      "Train Epoch: 1 [47/526 (9%)]\tLoss: 23.632439\tTotal Loss: 10.732420\n",
      "Train Epoch: 1 [48/526 (9%)]\tLoss: 15.062048\tTotal Loss: 10.822620\n",
      "Train Epoch: 1 [49/526 (9%)]\tLoss: 11.760968\tTotal Loss: 10.841770\n",
      "Train Epoch: 1 [50/526 (9%)]\tLoss: 34.638882\tTotal Loss: 11.317710\n",
      "Train Epoch: 1 [51/526 (10%)]\tLoss: 31.234447\tTotal Loss: 11.708230\n",
      "Train Epoch: 1 [52/526 (10%)]\tLoss: 0.000062\tTotal Loss: 11.483080\n",
      "Train Epoch: 1 [53/526 (10%)]\tLoss: 19.252615\tTotal Loss: 11.629670\n",
      "Train Epoch: 1 [54/526 (10%)]\tLoss: 19.063894\tTotal Loss: 11.767340\n",
      "Train Epoch: 1 [55/526 (10%)]\tLoss: 19.012642\tTotal Loss: 11.899070\n",
      "Train Epoch: 1 [56/526 (10%)]\tLoss: 17.360783\tTotal Loss: 11.996610\n",
      "Train Epoch: 1 [57/526 (11%)]\tLoss: 28.419563\tTotal Loss: 12.284730\n",
      "Train Epoch: 1 [58/526 (11%)]\tLoss: 25.055691\tTotal Loss: 12.504920\n",
      "Train Epoch: 1 [59/526 (11%)]\tLoss: 29.848480\tTotal Loss: 12.798880\n",
      "Train Epoch: 1 [60/526 (11%)]\tLoss: 17.206543\tTotal Loss: 12.872340\n",
      "Train Epoch: 1 [61/526 (11%)]\tLoss: 0.000000\tTotal Loss: 12.661310\n",
      "Train Epoch: 1 [62/526 (12%)]\tLoss: 14.150794\tTotal Loss: 12.685340\n",
      "Train Epoch: 1 [63/526 (12%)]\tLoss: 0.000726\tTotal Loss: 12.484000\n",
      "Train Epoch: 1 [64/526 (12%)]\tLoss: 13.383223\tTotal Loss: 12.498050\n",
      "Train Epoch: 1 [65/526 (12%)]\tLoss: 19.953993\tTotal Loss: 12.612750\n",
      "Train Epoch: 1 [66/526 (12%)]\tLoss: 28.020329\tTotal Loss: 12.846200\n",
      "Train Epoch: 1 [67/526 (13%)]\tLoss: 25.745182\tTotal Loss: 13.038720\n",
      "Train Epoch: 1 [68/526 (13%)]\tLoss: 18.082571\tTotal Loss: 13.112900\n",
      "Train Epoch: 1 [69/526 (13%)]\tLoss: 0.137722\tTotal Loss: 12.924850\n",
      "Train Epoch: 1 [70/526 (13%)]\tLoss: 11.584725\tTotal Loss: 12.905710\n",
      "Train Epoch: 1 [71/526 (13%)]\tLoss: 13.712704\tTotal Loss: 12.917070\n",
      "Train Epoch: 1 [72/526 (13%)]\tLoss: 41.223106\tTotal Loss: 13.310210\n",
      "Train Epoch: 1 [73/526 (14%)]\tLoss: 23.477533\tTotal Loss: 13.449490\n",
      "Train Epoch: 1 [74/526 (14%)]\tLoss: 43.295105\tTotal Loss: 13.852810\n",
      "Train Epoch: 1 [75/526 (14%)]\tLoss: 27.261665\tTotal Loss: 14.031590\n",
      "Train Epoch: 1 [76/526 (14%)]\tLoss: 10.084747\tTotal Loss: 13.979660\n",
      "Train Epoch: 1 [77/526 (14%)]\tLoss: 13.004477\tTotal Loss: 13.967000\n",
      "Train Epoch: 1 [78/526 (15%)]\tLoss: 0.000000\tTotal Loss: 13.787930\n",
      "Train Epoch: 1 [79/526 (15%)]\tLoss: 12.674756\tTotal Loss: 13.773840\n",
      "Train Epoch: 1 [80/526 (15%)]\tLoss: 1.362468\tTotal Loss: 13.618700\n",
      "Train Epoch: 1 [81/526 (15%)]\tLoss: 14.649110\tTotal Loss: 13.631420\n",
      "Train Epoch: 1 [82/526 (15%)]\tLoss: 18.078119\tTotal Loss: 13.685650\n",
      "Train Epoch: 1 [83/526 (16%)]\tLoss: 12.852172\tTotal Loss: 13.675610\n",
      "Train Epoch: 1 [84/526 (16%)]\tLoss: 4.175612\tTotal Loss: 13.562510\n",
      "Train Epoch: 1 [85/526 (16%)]\tLoss: 19.778870\tTotal Loss: 13.635650\n",
      "Train Epoch: 1 [86/526 (16%)]\tLoss: 23.703918\tTotal Loss: 13.752720\n",
      "Train Epoch: 1 [87/526 (16%)]\tLoss: 9.060616\tTotal Loss: 13.698790\n",
      "Train Epoch: 1 [88/526 (17%)]\tLoss: 34.875347\tTotal Loss: 13.939430\n",
      "Train Epoch: 1 [89/526 (17%)]\tLoss: 31.449615\tTotal Loss: 14.136170\n",
      "Train Epoch: 1 [90/526 (17%)]\tLoss: 18.261652\tTotal Loss: 14.182010\n",
      "Train Epoch: 1 [91/526 (17%)]\tLoss: 8.301296\tTotal Loss: 14.117390\n",
      "Train Epoch: 1 [92/526 (17%)]\tLoss: 33.313469\tTotal Loss: 14.326040\n",
      "Train Epoch: 1 [93/526 (17%)]\tLoss: 0.332967\tTotal Loss: 14.175580\n",
      "Train Epoch: 1 [94/526 (18%)]\tLoss: 49.992153\tTotal Loss: 14.556610\n",
      "Train Epoch: 1 [95/526 (18%)]\tLoss: 8.757149\tTotal Loss: 14.495560\n",
      "Train Epoch: 1 [96/526 (18%)]\tLoss: 6.419874\tTotal Loss: 14.411440\n",
      "Train Epoch: 1 [97/526 (18%)]\tLoss: 3.566193\tTotal Loss: 14.299630\n",
      "Train Epoch: 1 [98/526 (18%)]\tLoss: 19.823288\tTotal Loss: 14.355990\n",
      "Train Epoch: 1 [99/526 (19%)]\tLoss: 17.726801\tTotal Loss: 14.390040\n",
      "Train Epoch: 1 [100/526 (19%)]\tLoss: 16.298002\tTotal Loss: 14.409120\n",
      "Train Epoch: 1 [101/526 (19%)]\tLoss: 20.990641\tTotal Loss: 14.474290\n",
      "Train Epoch: 1 [102/526 (19%)]\tLoss: 35.688675\tTotal Loss: 14.682270\n",
      "Train Epoch: 1 [103/526 (19%)]\tLoss: 29.654228\tTotal Loss: 14.827630\n",
      "Train Epoch: 1 [104/526 (20%)]\tLoss: 30.644547\tTotal Loss: 14.979710\n",
      "Train Epoch: 1 [105/526 (20%)]\tLoss: 22.389189\tTotal Loss: 15.050280\n",
      "Train Epoch: 1 [106/526 (20%)]\tLoss: 24.372232\tTotal Loss: 15.138220\n",
      "Train Epoch: 1 [107/526 (20%)]\tLoss: 20.698412\tTotal Loss: 15.190190\n",
      "Train Epoch: 1 [108/526 (20%)]\tLoss: 12.496319\tTotal Loss: 15.165250\n",
      "Train Epoch: 1 [109/526 (21%)]\tLoss: 17.399847\tTotal Loss: 15.185750\n",
      "Train Epoch: 1 [110/526 (21%)]\tLoss: 34.823357\tTotal Loss: 15.364270\n",
      "Train Epoch: 1 [111/526 (21%)]\tLoss: 34.787811\tTotal Loss: 15.539260\n",
      "Train Epoch: 1 [112/526 (21%)]\tLoss: 1.235932\tTotal Loss: 15.411550\n",
      "Train Epoch: 1 [113/526 (21%)]\tLoss: 36.467110\tTotal Loss: 15.597880\n",
      "Train Epoch: 1 [114/526 (21%)]\tLoss: 16.499893\tTotal Loss: 15.605790\n",
      "Train Epoch: 1 [115/526 (22%)]\tLoss: 21.886005\tTotal Loss: 15.660400\n",
      "Train Epoch: 1 [116/526 (22%)]\tLoss: 21.206202\tTotal Loss: 15.708210\n",
      "Train Epoch: 1 [117/526 (22%)]\tLoss: 21.185286\tTotal Loss: 15.755020\n",
      "Train Epoch: 1 [118/526 (22%)]\tLoss: 13.539361\tTotal Loss: 15.736250\n",
      "Train Epoch: 1 [119/526 (22%)]\tLoss: 36.111217\tTotal Loss: 15.907470\n",
      "Train Epoch: 1 [120/526 (23%)]\tLoss: 17.376547\tTotal Loss: 15.919710\n",
      "Train Epoch: 1 [121/526 (23%)]\tLoss: 15.438687\tTotal Loss: 15.915730\n",
      "Train Epoch: 1 [122/526 (23%)]\tLoss: 23.512907\tTotal Loss: 15.978010\n",
      "Train Epoch: 1 [123/526 (23%)]\tLoss: 20.982203\tTotal Loss: 16.018690\n",
      "Train Epoch: 1 [124/526 (23%)]\tLoss: 36.616287\tTotal Loss: 16.184800\n",
      "Train Epoch: 1 [125/526 (24%)]\tLoss: 11.864815\tTotal Loss: 16.150240\n",
      "Train Epoch: 1 [126/526 (24%)]\tLoss: 25.631403\tTotal Loss: 16.225490\n",
      "Train Epoch: 1 [127/526 (24%)]\tLoss: 18.846235\tTotal Loss: 16.246120\n",
      "Train Epoch: 1 [128/526 (24%)]\tLoss: 26.946886\tTotal Loss: 16.329720\n",
      "Train Epoch: 1 [129/526 (24%)]\tLoss: 47.604275\tTotal Loss: 16.572160\n",
      "Train Epoch: 1 [130/526 (25%)]\tLoss: 6.663045\tTotal Loss: 16.495940\n",
      "Train Epoch: 1 [131/526 (25%)]\tLoss: 15.131599\tTotal Loss: 16.485520\n",
      "Train Epoch: 1 [132/526 (25%)]\tLoss: 22.981304\tTotal Loss: 16.534730\n",
      "Train Epoch: 1 [133/526 (25%)]\tLoss: 0.164540\tTotal Loss: 16.411650\n",
      "Train Epoch: 1 [134/526 (25%)]\tLoss: 19.578760\tTotal Loss: 16.435280\n",
      "Train Epoch: 1 [135/526 (25%)]\tLoss: 19.837740\tTotal Loss: 16.460490\n",
      "Train Epoch: 1 [136/526 (26%)]\tLoss: 32.130642\tTotal Loss: 16.575710\n",
      "Train Epoch: 1 [137/526 (26%)]\tLoss: 33.803371\tTotal Loss: 16.701460\n",
      "Train Epoch: 1 [138/526 (26%)]\tLoss: 30.622005\tTotal Loss: 16.802330\n",
      "Train Epoch: 1 [139/526 (26%)]\tLoss: 31.296978\tTotal Loss: 16.906610\n",
      "Train Epoch: 1 [140/526 (26%)]\tLoss: 23.188036\tTotal Loss: 16.951480\n",
      "Train Epoch: 1 [141/526 (27%)]\tLoss: 14.614647\tTotal Loss: 16.934900\n",
      "Train Epoch: 1 [142/526 (27%)]\tLoss: 28.415524\tTotal Loss: 17.015750\n",
      "Train Epoch: 1 [143/526 (27%)]\tLoss: 9.612759\tTotal Loss: 16.963980\n",
      "Train Epoch: 1 [144/526 (27%)]\tLoss: 29.740013\tTotal Loss: 17.052710\n",
      "Train Epoch: 1 [145/526 (27%)]\tLoss: 22.309294\tTotal Loss: 17.088960\n",
      "Train Epoch: 1 [146/526 (28%)]\tLoss: 34.012169\tTotal Loss: 17.204870\n",
      "Train Epoch: 1 [147/526 (28%)]\tLoss: 20.333633\tTotal Loss: 17.226160\n",
      "Train Epoch: 1 [148/526 (28%)]\tLoss: 67.498291\tTotal Loss: 17.565830\n",
      "Train Epoch: 1 [149/526 (28%)]\tLoss: 21.565086\tTotal Loss: 17.592670\n",
      "Train Epoch: 1 [150/526 (28%)]\tLoss: 30.514479\tTotal Loss: 17.678820\n",
      "Train Epoch: 1 [151/526 (29%)]\tLoss: 20.012583\tTotal Loss: 17.694270\n",
      "Train Epoch: 1 [152/526 (29%)]\tLoss: 35.623493\tTotal Loss: 17.812230\n",
      "Train Epoch: 1 [153/526 (29%)]\tLoss: 30.254864\tTotal Loss: 17.893550\n",
      "Train Epoch: 1 [154/526 (29%)]\tLoss: 7.577093\tTotal Loss: 17.826560\n",
      "Train Epoch: 1 [155/526 (29%)]\tLoss: 16.952559\tTotal Loss: 17.820920\n",
      "Train Epoch: 1 [156/526 (29%)]\tLoss: 22.811460\tTotal Loss: 17.852910\n",
      "Train Epoch: 1 [157/526 (30%)]\tLoss: 15.768209\tTotal Loss: 17.839640\n",
      "Train Epoch: 1 [158/526 (30%)]\tLoss: 22.605343\tTotal Loss: 17.869800\n",
      "Train Epoch: 1 [159/526 (30%)]\tLoss: 0.000000\tTotal Loss: 17.757410\n",
      "Train Epoch: 1 [160/526 (30%)]\tLoss: 42.386814\tTotal Loss: 17.911340\n",
      "Train Epoch: 1 [161/526 (30%)]\tLoss: 21.345779\tTotal Loss: 17.932680\n",
      "Train Epoch: 1 [162/526 (31%)]\tLoss: 26.417294\tTotal Loss: 17.985050\n",
      "Train Epoch: 1 [163/526 (31%)]\tLoss: 17.844530\tTotal Loss: 17.984190\n",
      "Train Epoch: 1 [164/526 (31%)]\tLoss: 27.428436\tTotal Loss: 18.041780\n",
      "Train Epoch: 1 [165/526 (31%)]\tLoss: 25.182728\tTotal Loss: 18.085050\n",
      "Train Epoch: 1 [166/526 (31%)]\tLoss: 36.505424\tTotal Loss: 18.196020\n",
      "Train Epoch: 1 [167/526 (32%)]\tLoss: 34.954144\tTotal Loss: 18.296370\n",
      "Train Epoch: 1 [168/526 (32%)]\tLoss: 6.113298\tTotal Loss: 18.223850\n",
      "Train Epoch: 1 [169/526 (32%)]\tLoss: 25.483809\tTotal Loss: 18.266810\n",
      "Train Epoch: 1 [170/526 (32%)]\tLoss: 23.254238\tTotal Loss: 18.296150\n",
      "Train Epoch: 1 [171/526 (32%)]\tLoss: 15.604712\tTotal Loss: 18.280410\n",
      "Train Epoch: 1 [172/526 (33%)]\tLoss: 37.629684\tTotal Loss: 18.392900\n",
      "Train Epoch: 1 [173/526 (33%)]\tLoss: 29.191420\tTotal Loss: 18.455320\n",
      "Train Epoch: 1 [174/526 (33%)]\tLoss: 24.502020\tTotal Loss: 18.490070\n",
      "Train Epoch: 1 [175/526 (33%)]\tLoss: 0.050567\tTotal Loss: 18.384700\n",
      "Train Epoch: 1 [176/526 (33%)]\tLoss: 20.313246\tTotal Loss: 18.395660\n",
      "Train Epoch: 1 [177/526 (33%)]\tLoss: 25.086521\tTotal Loss: 18.433460\n",
      "Train Epoch: 1 [178/526 (34%)]\tLoss: 20.534264\tTotal Loss: 18.445270\n",
      "Train Epoch: 1 [179/526 (34%)]\tLoss: 11.694840\tTotal Loss: 18.407550\n",
      "Train Epoch: 1 [180/526 (34%)]\tLoss: 2.106085\tTotal Loss: 18.316990\n",
      "Train Epoch: 1 [181/526 (34%)]\tLoss: 19.756168\tTotal Loss: 18.324940\n",
      "Train Epoch: 1 [182/526 (34%)]\tLoss: 7.116327\tTotal Loss: 18.263360\n",
      "Train Epoch: 1 [183/526 (35%)]\tLoss: 0.000121\tTotal Loss: 18.163560\n",
      "Train Epoch: 1 [184/526 (35%)]\tLoss: 20.816931\tTotal Loss: 18.177980\n",
      "Train Epoch: 1 [185/526 (35%)]\tLoss: 18.854082\tTotal Loss: 18.181630\n",
      "Train Epoch: 1 [186/526 (35%)]\tLoss: 30.526672\tTotal Loss: 18.248000\n",
      "Train Epoch: 1 [187/526 (35%)]\tLoss: 17.802383\tTotal Loss: 18.245620\n",
      "Train Epoch: 1 [188/526 (36%)]\tLoss: 19.051907\tTotal Loss: 18.249910\n",
      "Train Epoch: 1 [189/526 (36%)]\tLoss: 6.884263\tTotal Loss: 18.189770\n",
      "Train Epoch: 1 [190/526 (36%)]\tLoss: 20.139894\tTotal Loss: 18.200040\n",
      "Train Epoch: 1 [191/526 (36%)]\tLoss: 0.000012\tTotal Loss: 18.104750\n",
      "Train Epoch: 1 [192/526 (36%)]\tLoss: 53.200768\tTotal Loss: 18.287540\n",
      "Train Epoch: 1 [193/526 (37%)]\tLoss: 16.061958\tTotal Loss: 18.276010\n",
      "Train Epoch: 1 [194/526 (37%)]\tLoss: 34.198959\tTotal Loss: 18.358090\n",
      "Train Epoch: 1 [195/526 (37%)]\tLoss: 44.734562\tTotal Loss: 18.493350\n",
      "Train Epoch: 1 [196/526 (37%)]\tLoss: 11.190091\tTotal Loss: 18.456090\n",
      "Train Epoch: 1 [197/526 (37%)]\tLoss: 27.281939\tTotal Loss: 18.500890\n",
      "Train Epoch: 1 [198/526 (37%)]\tLoss: 24.393263\tTotal Loss: 18.530650\n",
      "Train Epoch: 1 [199/526 (38%)]\tLoss: 22.177076\tTotal Loss: 18.548970\n",
      "Train Epoch: 1 [200/526 (38%)]\tLoss: 20.979673\tTotal Loss: 18.561130\n",
      "Train Epoch: 1 [201/526 (38%)]\tLoss: 36.620026\tTotal Loss: 18.650970\n",
      "Train Epoch: 1 [202/526 (38%)]\tLoss: 31.672523\tTotal Loss: 18.715430\n",
      "Train Epoch: 1 [203/526 (38%)]\tLoss: 21.611698\tTotal Loss: 18.729700\n",
      "Train Epoch: 1 [204/526 (39%)]\tLoss: 27.514805\tTotal Loss: 18.772770\n",
      "Train Epoch: 1 [205/526 (39%)]\tLoss: 18.508638\tTotal Loss: 18.771480\n",
      "Train Epoch: 1 [206/526 (39%)]\tLoss: 10.661541\tTotal Loss: 18.732110\n",
      "Train Epoch: 1 [207/526 (39%)]\tLoss: 24.550995\tTotal Loss: 18.760220\n",
      "Train Epoch: 1 [208/526 (39%)]\tLoss: 30.556437\tTotal Loss: 18.816930\n",
      "Train Epoch: 1 [209/526 (40%)]\tLoss: 33.833958\tTotal Loss: 18.888780\n",
      "Train Epoch: 1 [210/526 (40%)]\tLoss: 36.968773\tTotal Loss: 18.974880\n",
      "Train Epoch: 1 [211/526 (40%)]\tLoss: 14.252184\tTotal Loss: 18.952500\n",
      "Train Epoch: 1 [212/526 (40%)]\tLoss: 28.642246\tTotal Loss: 18.998200\n",
      "Train Epoch: 1 [213/526 (40%)]\tLoss: 24.581871\tTotal Loss: 19.024420\n",
      "Train Epoch: 1 [214/526 (40%)]\tLoss: 0.000000\tTotal Loss: 18.935520\n",
      "Train Epoch: 1 [215/526 (41%)]\tLoss: 12.963764\tTotal Loss: 18.907740\n",
      "Train Epoch: 1 [216/526 (41%)]\tLoss: 21.710146\tTotal Loss: 18.920720\n",
      "Train Epoch: 1 [217/526 (41%)]\tLoss: 18.275482\tTotal Loss: 18.917740\n",
      "Train Epoch: 1 [218/526 (41%)]\tLoss: 63.298275\tTotal Loss: 19.121320\n",
      "Train Epoch: 1 [219/526 (41%)]\tLoss: 30.943098\tTotal Loss: 19.175300\n",
      "Train Epoch: 1 [220/526 (42%)]\tLoss: 17.764717\tTotal Loss: 19.168890\n",
      "Train Epoch: 1 [221/526 (42%)]\tLoss: 16.861237\tTotal Loss: 19.158450\n",
      "Train Epoch: 1 [222/526 (42%)]\tLoss: 29.861589\tTotal Loss: 19.206660\n",
      "Train Epoch: 1 [223/526 (42%)]\tLoss: 63.124500\tTotal Loss: 19.403600\n",
      "Train Epoch: 1 [224/526 (42%)]\tLoss: 19.270409\tTotal Loss: 19.403010\n",
      "Train Epoch: 1 [225/526 (43%)]\tLoss: 16.300585\tTotal Loss: 19.389220\n",
      "Train Epoch: 1 [226/526 (43%)]\tLoss: 27.990799\tTotal Loss: 19.427280\n",
      "Train Epoch: 1 [227/526 (43%)]\tLoss: 33.819347\tTotal Loss: 19.490680\n",
      "Train Epoch: 1 [228/526 (43%)]\tLoss: 21.619057\tTotal Loss: 19.500020\n",
      "Train Epoch: 1 [229/526 (43%)]\tLoss: 19.253983\tTotal Loss: 19.498940\n",
      "Train Epoch: 1 [230/526 (44%)]\tLoss: 3.602123\tTotal Loss: 19.429830\n",
      "Train Epoch: 1 [231/526 (44%)]\tLoss: 20.165144\tTotal Loss: 19.433010\n",
      "Train Epoch: 1 [232/526 (44%)]\tLoss: 10.132633\tTotal Loss: 19.392920\n",
      "Train Epoch: 1 [233/526 (44%)]\tLoss: 24.386839\tTotal Loss: 19.414350\n",
      "Train Epoch: 1 [234/526 (44%)]\tLoss: 28.183292\tTotal Loss: 19.451830\n",
      "Train Epoch: 1 [235/526 (44%)]\tLoss: 14.630261\tTotal Loss: 19.431310\n",
      "Train Epoch: 1 [236/526 (45%)]\tLoss: 32.073948\tTotal Loss: 19.484880\n",
      "Train Epoch: 1 [237/526 (45%)]\tLoss: 71.504898\tTotal Loss: 19.704380\n",
      "Train Epoch: 1 [238/526 (45%)]\tLoss: 24.525883\tTotal Loss: 19.724630\n",
      "Train Epoch: 1 [239/526 (45%)]\tLoss: 11.347570\tTotal Loss: 19.689580\n",
      "Train Epoch: 1 [240/526 (45%)]\tLoss: 21.763727\tTotal Loss: 19.698230\n",
      "Train Epoch: 1 [241/526 (46%)]\tLoss: 37.142975\tTotal Loss: 19.770610\n",
      "Train Epoch: 1 [242/526 (46%)]\tLoss: 33.818962\tTotal Loss: 19.828660\n",
      "Train Epoch: 1 [243/526 (46%)]\tLoss: 28.308718\tTotal Loss: 19.863560\n",
      "Train Epoch: 1 [244/526 (46%)]\tLoss: 49.001320\tTotal Loss: 19.982980\n",
      "Train Epoch: 1 [245/526 (46%)]\tLoss: 40.355026\tTotal Loss: 20.066130\n",
      "Train Epoch: 1 [246/526 (47%)]\tLoss: 27.227280\tTotal Loss: 20.095240\n",
      "Train Epoch: 1 [247/526 (47%)]\tLoss: 21.965429\tTotal Loss: 20.102810\n",
      "Train Epoch: 1 [248/526 (47%)]\tLoss: 7.388021\tTotal Loss: 20.051540\n",
      "Train Epoch: 1 [249/526 (47%)]\tLoss: 24.790407\tTotal Loss: 20.070570\n",
      "Train Epoch: 1 [250/526 (47%)]\tLoss: 16.276133\tTotal Loss: 20.055390\n",
      "Train Epoch: 1 [251/526 (48%)]\tLoss: 40.780178\tTotal Loss: 20.137960\n",
      "Train Epoch: 1 [252/526 (48%)]\tLoss: 24.389469\tTotal Loss: 20.154830\n",
      "Train Epoch: 1 [253/526 (48%)]\tLoss: 57.546402\tTotal Loss: 20.302630\n",
      "Train Epoch: 1 [254/526 (48%)]\tLoss: 43.234116\tTotal Loss: 20.392910\n",
      "Train Epoch: 1 [255/526 (48%)]\tLoss: 33.869644\tTotal Loss: 20.445760\n",
      "Train Epoch: 1 [256/526 (48%)]\tLoss: 21.407207\tTotal Loss: 20.449510\n",
      "Train Epoch: 1 [257/526 (49%)]\tLoss: 23.883831\tTotal Loss: 20.462880\n",
      "Train Epoch: 1 [258/526 (49%)]\tLoss: 73.785187\tTotal Loss: 20.669550\n",
      "Train Epoch: 1 [259/526 (49%)]\tLoss: 32.036530\tTotal Loss: 20.713440\n",
      "Train Epoch: 1 [260/526 (49%)]\tLoss: 0.000016\tTotal Loss: 20.633770\n",
      "Train Epoch: 1 [261/526 (49%)]\tLoss: 19.615898\tTotal Loss: 20.629870\n",
      "Train Epoch: 1 [262/526 (50%)]\tLoss: 20.976620\tTotal Loss: 20.631200\n",
      "Train Epoch: 1 [263/526 (50%)]\tLoss: 32.032562\tTotal Loss: 20.674550\n",
      "Train Epoch: 1 [264/526 (50%)]\tLoss: 18.951027\tTotal Loss: 20.668020\n",
      "Train Epoch: 1 [265/526 (50%)]\tLoss: 33.242317\tTotal Loss: 20.715470\n",
      "Train Epoch: 1 [266/526 (50%)]\tLoss: 20.282364\tTotal Loss: 20.713840\n",
      "Train Epoch: 1 [267/526 (51%)]\tLoss: 25.809429\tTotal Loss: 20.732930\n",
      "Train Epoch: 1 [268/526 (51%)]\tLoss: 34.762280\tTotal Loss: 20.785270\n",
      "Train Epoch: 1 [269/526 (51%)]\tLoss: 26.814388\tTotal Loss: 20.807690\n",
      "Train Epoch: 1 [270/526 (51%)]\tLoss: 24.509098\tTotal Loss: 20.821400\n",
      "Train Epoch: 1 [271/526 (51%)]\tLoss: 22.001923\tTotal Loss: 20.825750\n",
      "Train Epoch: 1 [272/526 (52%)]\tLoss: 11.093263\tTotal Loss: 20.789970\n",
      "Train Epoch: 1 [273/526 (52%)]\tLoss: 22.165178\tTotal Loss: 20.795010\n",
      "Train Epoch: 1 [274/526 (52%)]\tLoss: 19.759090\tTotal Loss: 20.791230\n",
      "Train Epoch: 1 [275/526 (52%)]\tLoss: 33.187332\tTotal Loss: 20.836300\n",
      "Train Epoch: 1 [276/526 (52%)]\tLoss: 13.366493\tTotal Loss: 20.809240\n",
      "Train Epoch: 1 [277/526 (52%)]\tLoss: 34.511078\tTotal Loss: 20.858710\n",
      "Train Epoch: 1 [278/526 (53%)]\tLoss: 59.173874\tTotal Loss: 20.996530\n",
      "Train Epoch: 1 [279/526 (53%)]\tLoss: 56.615032\tTotal Loss: 21.124190\n",
      "Train Epoch: 1 [280/526 (53%)]\tLoss: 27.770472\tTotal Loss: 21.147930\n",
      "Train Epoch: 1 [281/526 (53%)]\tLoss: 0.001005\tTotal Loss: 21.072680\n",
      "Train Epoch: 1 [282/526 (53%)]\tLoss: 38.516449\tTotal Loss: 21.134530\n",
      "Train Epoch: 1 [283/526 (54%)]\tLoss: 60.607048\tTotal Loss: 21.274010\n",
      "Train Epoch: 1 [284/526 (54%)]\tLoss: 39.242664\tTotal Loss: 21.337280\n",
      "Train Epoch: 1 [285/526 (54%)]\tLoss: 19.225935\tTotal Loss: 21.329870\n",
      "Train Epoch: 1 [286/526 (54%)]\tLoss: 26.324440\tTotal Loss: 21.347340\n",
      "Train Epoch: 1 [287/526 (54%)]\tLoss: 21.419268\tTotal Loss: 21.347590\n",
      "Train Epoch: 1 [288/526 (55%)]\tLoss: 37.094555\tTotal Loss: 21.402260\n",
      "Train Epoch: 1 [289/526 (55%)]\tLoss: 26.194073\tTotal Loss: 21.418840\n",
      "Train Epoch: 1 [290/526 (55%)]\tLoss: 0.000003\tTotal Loss: 21.344990\n",
      "Train Epoch: 1 [291/526 (55%)]\tLoss: 26.270151\tTotal Loss: 21.361910\n",
      "Train Epoch: 1 [292/526 (55%)]\tLoss: 18.716265\tTotal Loss: 21.352850\n",
      "Train Epoch: 1 [293/526 (56%)]\tLoss: 24.454657\tTotal Loss: 21.363440\n",
      "Train Epoch: 1 [294/526 (56%)]\tLoss: 19.037905\tTotal Loss: 21.355530\n",
      "Train Epoch: 1 [295/526 (56%)]\tLoss: 42.072865\tTotal Loss: 21.425760\n",
      "Train Epoch: 1 [296/526 (56%)]\tLoss: 17.361689\tTotal Loss: 21.412030\n",
      "Train Epoch: 1 [297/526 (56%)]\tLoss: 32.529808\tTotal Loss: 21.449460\n",
      "Train Epoch: 1 [298/526 (56%)]\tLoss: 26.670675\tTotal Loss: 21.466980\n",
      "Train Epoch: 1 [299/526 (57%)]\tLoss: 26.145697\tTotal Loss: 21.482630\n",
      "Train Epoch: 1 [300/526 (57%)]\tLoss: 26.544252\tTotal Loss: 21.499500\n",
      "Train Epoch: 1 [301/526 (57%)]\tLoss: 17.607876\tTotal Loss: 21.486570\n",
      "Train Epoch: 1 [302/526 (57%)]\tLoss: 36.575256\tTotal Loss: 21.536530\n",
      "Train Epoch: 1 [303/526 (57%)]\tLoss: 15.228978\tTotal Loss: 21.515720\n",
      "Train Epoch: 1 [304/526 (58%)]\tLoss: 31.780043\tTotal Loss: 21.549480\n",
      "Train Epoch: 1 [305/526 (58%)]\tLoss: 11.810055\tTotal Loss: 21.517550\n",
      "Train Epoch: 1 [306/526 (58%)]\tLoss: 33.905872\tTotal Loss: 21.558030\n",
      "Train Epoch: 1 [307/526 (58%)]\tLoss: 27.234545\tTotal Loss: 21.576520\n",
      "Train Epoch: 1 [308/526 (58%)]\tLoss: 34.283234\tTotal Loss: 21.617780\n",
      "Train Epoch: 1 [309/526 (59%)]\tLoss: 27.844303\tTotal Loss: 21.637930\n",
      "Train Epoch: 1 [310/526 (59%)]\tLoss: 10.668002\tTotal Loss: 21.602540\n",
      "Train Epoch: 1 [311/526 (59%)]\tLoss: 21.278543\tTotal Loss: 21.601500\n",
      "Train Epoch: 1 [312/526 (59%)]\tLoss: 38.101116\tTotal Loss: 21.654380\n",
      "Train Epoch: 1 [313/526 (59%)]\tLoss: 21.773809\tTotal Loss: 21.654770\n",
      "Train Epoch: 1 [314/526 (60%)]\tLoss: 53.501835\tTotal Loss: 21.756190\n",
      "Train Epoch: 1 [315/526 (60%)]\tLoss: 22.895803\tTotal Loss: 21.759810\n",
      "Train Epoch: 1 [316/526 (60%)]\tLoss: 49.356297\tTotal Loss: 21.847140\n",
      "Train Epoch: 1 [317/526 (60%)]\tLoss: 21.195679\tTotal Loss: 21.845080\n",
      "Train Epoch: 1 [318/526 (60%)]\tLoss: 25.245237\tTotal Loss: 21.855780\n",
      "Train Epoch: 1 [319/526 (60%)]\tLoss: 9.970266\tTotal Loss: 21.818520\n",
      "Train Epoch: 1 [320/526 (61%)]\tLoss: 62.895103\tTotal Loss: 21.946880\n",
      "Train Epoch: 1 [321/526 (61%)]\tLoss: 37.310429\tTotal Loss: 21.994740\n",
      "Train Epoch: 1 [322/526 (61%)]\tLoss: 45.308571\tTotal Loss: 22.067150\n",
      "Train Epoch: 1 [323/526 (61%)]\tLoss: 21.369263\tTotal Loss: 22.064990\n",
      "Train Epoch: 1 [324/526 (61%)]\tLoss: 12.895550\tTotal Loss: 22.036680\n",
      "Train Epoch: 1 [325/526 (62%)]\tLoss: 17.220316\tTotal Loss: 22.021860\n",
      "Train Epoch: 1 [326/526 (62%)]\tLoss: 32.435608\tTotal Loss: 22.053810\n",
      "Train Epoch: 1 [327/526 (62%)]\tLoss: 53.554497\tTotal Loss: 22.150140\n",
      "Train Epoch: 1 [328/526 (62%)]\tLoss: 38.197254\tTotal Loss: 22.199070\n",
      "Train Epoch: 1 [329/526 (62%)]\tLoss: 13.045804\tTotal Loss: 22.171240\n",
      "Train Epoch: 1 [330/526 (63%)]\tLoss: 0.000012\tTotal Loss: 22.104060\n",
      "Train Epoch: 1 [331/526 (63%)]\tLoss: 28.733099\tTotal Loss: 22.124090\n",
      "Train Epoch: 1 [332/526 (63%)]\tLoss: 20.244490\tTotal Loss: 22.118420\n",
      "Train Epoch: 1 [333/526 (63%)]\tLoss: 1.648256\tTotal Loss: 22.056950\n",
      "Train Epoch: 1 [334/526 (63%)]\tLoss: 19.360390\tTotal Loss: 22.048880\n",
      "Train Epoch: 1 [335/526 (63%)]\tLoss: 18.912350\tTotal Loss: 22.039520\n",
      "Train Epoch: 1 [336/526 (64%)]\tLoss: 5.869359\tTotal Loss: 21.991390\n",
      "Train Epoch: 1 [337/526 (64%)]\tLoss: 57.070869\tTotal Loss: 22.095480\n",
      "Train Epoch: 1 [338/526 (64%)]\tLoss: 22.410694\tTotal Loss: 22.096420\n",
      "Train Epoch: 1 [339/526 (64%)]\tLoss: 0.000000\tTotal Loss: 22.031240\n",
      "Train Epoch: 1 [340/526 (64%)]\tLoss: 12.079497\tTotal Loss: 22.001970\n",
      "Train Epoch: 1 [341/526 (65%)]\tLoss: 26.596882\tTotal Loss: 22.015440\n",
      "Train Epoch: 1 [342/526 (65%)]\tLoss: 17.532364\tTotal Loss: 22.002330\n",
      "Train Epoch: 1 [343/526 (65%)]\tLoss: 34.573856\tTotal Loss: 22.038980\n",
      "Train Epoch: 1 [344/526 (65%)]\tLoss: 9.500327\tTotal Loss: 22.002530\n",
      "Train Epoch: 1 [345/526 (65%)]\tLoss: 0.000414\tTotal Loss: 21.938760\n",
      "Train Epoch: 1 [346/526 (66%)]\tLoss: 37.774361\tTotal Loss: 21.984530\n",
      "Train Epoch: 1 [347/526 (66%)]\tLoss: 33.704994\tTotal Loss: 22.018300\n",
      "Train Epoch: 1 [348/526 (66%)]\tLoss: 6.474935\tTotal Loss: 21.973640\n",
      "Train Epoch: 1 [349/526 (66%)]\tLoss: 46.862762\tTotal Loss: 22.044950\n",
      "Train Epoch: 1 [350/526 (66%)]\tLoss: 35.989521\tTotal Loss: 22.084800\n",
      "Train Epoch: 1 [351/526 (67%)]\tLoss: 58.892517\tTotal Loss: 22.189660\n",
      "Train Epoch: 1 [352/526 (67%)]\tLoss: 27.046333\tTotal Loss: 22.203460\n",
      "Train Epoch: 1 [353/526 (67%)]\tLoss: 34.969860\tTotal Loss: 22.239620\n",
      "Train Epoch: 1 [354/526 (67%)]\tLoss: 28.581903\tTotal Loss: 22.257540\n",
      "Train Epoch: 1 [355/526 (67%)]\tLoss: 13.275616\tTotal Loss: 22.232240\n",
      "Train Epoch: 1 [356/526 (67%)]\tLoss: 20.915791\tTotal Loss: 22.228540\n",
      "Train Epoch: 1 [357/526 (68%)]\tLoss: 21.848753\tTotal Loss: 22.227480\n",
      "Train Epoch: 1 [358/526 (68%)]\tLoss: 0.000000\tTotal Loss: 22.165390\n",
      "Train Epoch: 1 [359/526 (68%)]\tLoss: 25.029234\tTotal Loss: 22.173370\n",
      "Train Epoch: 1 [360/526 (68%)]\tLoss: 44.965019\tTotal Loss: 22.236680\n",
      "Train Epoch: 1 [361/526 (68%)]\tLoss: 22.553396\tTotal Loss: 22.237550\n",
      "Train Epoch: 1 [362/526 (69%)]\tLoss: 26.824808\tTotal Loss: 22.250230\n",
      "Train Epoch: 1 [363/526 (69%)]\tLoss: 17.408691\tTotal Loss: 22.236890\n",
      "Train Epoch: 1 [364/526 (69%)]\tLoss: 44.254044\tTotal Loss: 22.297380\n",
      "Train Epoch: 1 [365/526 (69%)]\tLoss: 42.881577\tTotal Loss: 22.353770\n",
      "Train Epoch: 1 [366/526 (69%)]\tLoss: 21.375124\tTotal Loss: 22.351100\n",
      "Train Epoch: 1 [367/526 (70%)]\tLoss: 36.633984\tTotal Loss: 22.390010\n",
      "Train Epoch: 1 [368/526 (70%)]\tLoss: 25.616364\tTotal Loss: 22.398780\n",
      "Train Epoch: 1 [369/526 (70%)]\tLoss: 73.793869\tTotal Loss: 22.538060\n",
      "Train Epoch: 1 [370/526 (70%)]\tLoss: 21.344465\tTotal Loss: 22.534840\n",
      "Train Epoch: 1 [371/526 (70%)]\tLoss: 0.000016\tTotal Loss: 22.474100\n",
      "Train Epoch: 1 [372/526 (71%)]\tLoss: 35.798477\tTotal Loss: 22.509920\n",
      "Train Epoch: 1 [373/526 (71%)]\tLoss: 52.257790\tTotal Loss: 22.589670\n",
      "Train Epoch: 1 [374/526 (71%)]\tLoss: 24.415022\tTotal Loss: 22.594550\n",
      "Train Epoch: 1 [375/526 (71%)]\tLoss: 37.465080\tTotal Loss: 22.634200\n",
      "Train Epoch: 1 [376/526 (71%)]\tLoss: 30.049520\tTotal Loss: 22.653930\n",
      "Train Epoch: 1 [377/526 (71%)]\tLoss: 7.715227\tTotal Loss: 22.614300\n",
      "Train Epoch: 1 [378/526 (72%)]\tLoss: 22.927921\tTotal Loss: 22.615130\n",
      "Train Epoch: 1 [379/526 (72%)]\tLoss: 108.153961\tTotal Loss: 22.840830\n",
      "Train Epoch: 1 [380/526 (72%)]\tLoss: 24.720781\tTotal Loss: 22.845770\n",
      "Train Epoch: 1 [381/526 (72%)]\tLoss: 4.947340\tTotal Loss: 22.798800\n",
      "Train Epoch: 1 [382/526 (72%)]\tLoss: 13.795691\tTotal Loss: 22.775230\n",
      "Train Epoch: 1 [383/526 (73%)]\tLoss: 26.045254\tTotal Loss: 22.783770\n",
      "Train Epoch: 1 [384/526 (73%)]\tLoss: 0.000000\tTotal Loss: 22.724430\n",
      "Train Epoch: 1 [385/526 (73%)]\tLoss: 47.080631\tTotal Loss: 22.787700\n",
      "Train Epoch: 1 [386/526 (73%)]\tLoss: 0.308415\tTotal Loss: 22.729460\n",
      "Train Epoch: 1 [387/526 (73%)]\tLoss: 25.759209\tTotal Loss: 22.737290\n",
      "Train Epoch: 1 [388/526 (74%)]\tLoss: 48.536625\tTotal Loss: 22.803780\n",
      "Train Epoch: 1 [389/526 (74%)]\tLoss: 32.459255\tTotal Loss: 22.828600\n",
      "Train Epoch: 1 [390/526 (74%)]\tLoss: 36.521305\tTotal Loss: 22.863710\n",
      "Train Epoch: 1 [391/526 (74%)]\tLoss: 37.520714\tTotal Loss: 22.901200\n",
      "Train Epoch: 1 [392/526 (74%)]\tLoss: 26.736330\tTotal Loss: 22.910980\n",
      "Train Epoch: 1 [393/526 (75%)]\tLoss: 40.367165\tTotal Loss: 22.955400\n",
      "Train Epoch: 1 [394/526 (75%)]\tLoss: 9.336592\tTotal Loss: 22.920830\n",
      "Train Epoch: 1 [395/526 (75%)]\tLoss: 38.130508\tTotal Loss: 22.959340\n",
      "Train Epoch: 1 [396/526 (75%)]\tLoss: 16.931841\tTotal Loss: 22.944120\n",
      "Train Epoch: 1 [397/526 (75%)]\tLoss: 42.083740\tTotal Loss: 22.992330\n",
      "Train Epoch: 1 [398/526 (75%)]\tLoss: 24.005959\tTotal Loss: 22.994880\n",
      "Train Epoch: 1 [399/526 (76%)]\tLoss: 24.404449\tTotal Loss: 22.998410\n",
      "Train Epoch: 1 [400/526 (76%)]\tLoss: 38.284924\tTotal Loss: 23.036620\n",
      "Train Epoch: 1 [401/526 (76%)]\tLoss: 18.776175\tTotal Loss: 23.026000\n",
      "Train Epoch: 1 [402/526 (76%)]\tLoss: 37.048771\tTotal Loss: 23.060880\n",
      "Train Epoch: 1 [403/526 (76%)]\tLoss: 18.641821\tTotal Loss: 23.049920\n",
      "Train Epoch: 1 [404/526 (77%)]\tLoss: 27.532322\tTotal Loss: 23.061010\n",
      "Train Epoch: 1 [405/526 (77%)]\tLoss: 2.677847\tTotal Loss: 23.010680\n",
      "Train Epoch: 1 [406/526 (77%)]\tLoss: 24.053574\tTotal Loss: 23.013250\n",
      "Train Epoch: 1 [407/526 (77%)]\tLoss: 11.248003\tTotal Loss: 22.984340\n",
      "Train Epoch: 1 [408/526 (77%)]\tLoss: 24.051205\tTotal Loss: 22.986960\n",
      "Train Epoch: 1 [409/526 (78%)]\tLoss: 14.597263\tTotal Loss: 22.966450\n",
      "Train Epoch: 1 [410/526 (78%)]\tLoss: 43.784515\tTotal Loss: 23.017220\n",
      "Train Epoch: 1 [411/526 (78%)]\tLoss: 27.941607\tTotal Loss: 23.029200\n",
      "Train Epoch: 1 [412/526 (78%)]\tLoss: 23.445389\tTotal Loss: 23.030210\n",
      "Train Epoch: 1 [413/526 (78%)]\tLoss: 28.416285\tTotal Loss: 23.043260\n",
      "Train Epoch: 1 [414/526 (79%)]\tLoss: 28.691120\tTotal Loss: 23.056900\n",
      "Train Epoch: 1 [415/526 (79%)]\tLoss: 47.715622\tTotal Loss: 23.116320\n",
      "Train Epoch: 1 [416/526 (79%)]\tLoss: 28.407103\tTotal Loss: 23.129030\n",
      "Train Epoch: 1 [417/526 (79%)]\tLoss: 0.003645\tTotal Loss: 23.073580\n",
      "Train Epoch: 1 [418/526 (79%)]\tLoss: 26.412891\tTotal Loss: 23.081570\n",
      "Train Epoch: 1 [419/526 (79%)]\tLoss: 68.253418\tTotal Loss: 23.189380\n",
      "Train Epoch: 1 [420/526 (80%)]\tLoss: 42.714485\tTotal Loss: 23.235860\n",
      "Train Epoch: 1 [421/526 (80%)]\tLoss: 28.165985\tTotal Loss: 23.247570\n",
      "Train Epoch: 1 [422/526 (80%)]\tLoss: 20.438360\tTotal Loss: 23.240920\n",
      "Train Epoch: 1 [423/526 (80%)]\tLoss: 28.713646\tTotal Loss: 23.253860\n",
      "Train Epoch: 1 [424/526 (80%)]\tLoss: 24.513079\tTotal Loss: 23.256830\n",
      "Train Epoch: 1 [425/526 (81%)]\tLoss: 30.090088\tTotal Loss: 23.272900\n",
      "Train Epoch: 1 [426/526 (81%)]\tLoss: 42.021793\tTotal Loss: 23.316920\n",
      "Train Epoch: 1 [427/526 (81%)]\tLoss: 36.244644\tTotal Loss: 23.347190\n",
      "Train Epoch: 1 [428/526 (81%)]\tLoss: 25.810360\tTotal Loss: 23.352950\n",
      "Train Epoch: 1 [429/526 (81%)]\tLoss: 22.793955\tTotal Loss: 23.351640\n",
      "Train Epoch: 1 [430/526 (82%)]\tLoss: 24.814526\tTotal Loss: 23.355040\n",
      "Train Epoch: 1 [431/526 (82%)]\tLoss: 25.110168\tTotal Loss: 23.359120\n",
      "Train Epoch: 1 [432/526 (82%)]\tLoss: 5.560105\tTotal Loss: 23.317920\n",
      "Train Epoch: 1 [433/526 (82%)]\tLoss: 27.345627\tTotal Loss: 23.327220\n",
      "Train Epoch: 1 [434/526 (82%)]\tLoss: 10.177643\tTotal Loss: 23.296920\n",
      "Train Epoch: 1 [435/526 (83%)]\tLoss: 13.648819\tTotal Loss: 23.274740\n",
      "Train Epoch: 1 [436/526 (83%)]\tLoss: 27.119928\tTotal Loss: 23.283560\n",
      "Train Epoch: 1 [437/526 (83%)]\tLoss: 13.612710\tTotal Loss: 23.261430\n",
      "Train Epoch: 1 [438/526 (83%)]\tLoss: 18.946678\tTotal Loss: 23.251580\n",
      "Train Epoch: 1 [439/526 (83%)]\tLoss: 22.698988\tTotal Loss: 23.250320\n",
      "Train Epoch: 1 [440/526 (83%)]\tLoss: 61.416122\tTotal Loss: 23.337060\n",
      "Train Epoch: 1 [441/526 (84%)]\tLoss: 45.210140\tTotal Loss: 23.386660\n",
      "Train Epoch: 1 [442/526 (84%)]\tLoss: 54.506805\tTotal Loss: 23.457070\n",
      "Train Epoch: 1 [443/526 (84%)]\tLoss: 24.233871\tTotal Loss: 23.458820\n",
      "Train Epoch: 1 [444/526 (84%)]\tLoss: 47.527493\tTotal Loss: 23.513030\n",
      "Train Epoch: 1 [445/526 (84%)]\tLoss: 11.078084\tTotal Loss: 23.485080\n",
      "Train Epoch: 1 [446/526 (85%)]\tLoss: 30.018513\tTotal Loss: 23.499730\n",
      "Train Epoch: 1 [447/526 (85%)]\tLoss: 42.463108\tTotal Loss: 23.542160\n",
      "Train Epoch: 1 [448/526 (85%)]\tLoss: 37.396698\tTotal Loss: 23.573080\n",
      "Train Epoch: 1 [449/526 (85%)]\tLoss: 0.004148\tTotal Loss: 23.520590\n",
      "Train Epoch: 1 [450/526 (85%)]\tLoss: 23.256456\tTotal Loss: 23.520000\n",
      "Train Epoch: 1 [451/526 (86%)]\tLoss: 51.307335\tTotal Loss: 23.581620\n",
      "Train Epoch: 1 [452/526 (86%)]\tLoss: 28.984268\tTotal Loss: 23.593570\n",
      "Train Epoch: 1 [453/526 (86%)]\tLoss: 59.957863\tTotal Loss: 23.673840\n",
      "Train Epoch: 1 [454/526 (86%)]\tLoss: 22.935373\tTotal Loss: 23.672220\n",
      "Train Epoch: 1 [455/526 (86%)]\tLoss: 4.366787\tTotal Loss: 23.629790\n",
      "Train Epoch: 1 [456/526 (87%)]\tLoss: 19.874063\tTotal Loss: 23.621550\n",
      "Train Epoch: 1 [457/526 (87%)]\tLoss: 34.262760\tTotal Loss: 23.644840\n",
      "Train Epoch: 1 [458/526 (87%)]\tLoss: 52.494320\tTotal Loss: 23.707830\n",
      "Train Epoch: 1 [459/526 (87%)]\tLoss: 13.883446\tTotal Loss: 23.686420\n",
      "Train Epoch: 1 [460/526 (87%)]\tLoss: 35.555145\tTotal Loss: 23.712220\n",
      "Train Epoch: 1 [461/526 (87%)]\tLoss: 35.099579\tTotal Loss: 23.736920\n",
      "Train Epoch: 1 [462/526 (88%)]\tLoss: 33.630592\tTotal Loss: 23.758340\n",
      "Train Epoch: 1 [463/526 (88%)]\tLoss: 34.409237\tTotal Loss: 23.781340\n",
      "Train Epoch: 1 [464/526 (88%)]\tLoss: 0.000000\tTotal Loss: 23.730090\n",
      "Train Epoch: 1 [465/526 (88%)]\tLoss: 0.000005\tTotal Loss: 23.679060\n",
      "Train Epoch: 1 [466/526 (88%)]\tLoss: 40.526234\tTotal Loss: 23.715210\n",
      "Train Epoch: 1 [467/526 (89%)]\tLoss: 0.032424\tTotal Loss: 23.664500\n",
      "Train Epoch: 1 [468/526 (89%)]\tLoss: 16.939436\tTotal Loss: 23.650130\n",
      "Train Epoch: 1 [469/526 (89%)]\tLoss: 41.667423\tTotal Loss: 23.688550\n",
      "Train Epoch: 1 [470/526 (89%)]\tLoss: 23.301739\tTotal Loss: 23.687720\n",
      "Train Epoch: 1 [471/526 (89%)]\tLoss: 16.812101\tTotal Loss: 23.673120\n",
      "Train Epoch: 1 [472/526 (90%)]\tLoss: 20.294167\tTotal Loss: 23.665970\n",
      "Train Epoch: 1 [473/526 (90%)]\tLoss: 38.470413\tTotal Loss: 23.697260\n",
      "Train Epoch: 1 [474/526 (90%)]\tLoss: 4.493202\tTotal Loss: 23.656750\n",
      "Train Epoch: 1 [475/526 (90%)]\tLoss: 3.330338\tTotal Loss: 23.613960\n",
      "Train Epoch: 1 [476/526 (90%)]\tLoss: 27.452061\tTotal Loss: 23.622020\n",
      "Train Epoch: 1 [477/526 (90%)]\tLoss: 19.711102\tTotal Loss: 23.613820\n",
      "Train Epoch: 1 [478/526 (91%)]\tLoss: 34.460159\tTotal Loss: 23.636510\n",
      "Train Epoch: 1 [479/526 (91%)]\tLoss: 32.855614\tTotal Loss: 23.655760\n",
      "Train Epoch: 1 [480/526 (91%)]\tLoss: 0.027191\tTotal Loss: 23.606530\n",
      "Train Epoch: 1 [481/526 (91%)]\tLoss: 35.204845\tTotal Loss: 23.630650\n",
      "Train Epoch: 1 [482/526 (91%)]\tLoss: 18.637335\tTotal Loss: 23.620290\n",
      "Train Epoch: 1 [483/526 (92%)]\tLoss: 0.000000\tTotal Loss: 23.571380\n",
      "Train Epoch: 1 [484/526 (92%)]\tLoss: 24.311823\tTotal Loss: 23.572910\n",
      "Train Epoch: 1 [485/526 (92%)]\tLoss: 29.122437\tTotal Loss: 23.584360\n",
      "Train Epoch: 1 [486/526 (92%)]\tLoss: 37.180111\tTotal Loss: 23.612330\n",
      "Train Epoch: 1 [487/526 (92%)]\tLoss: 32.804714\tTotal Loss: 23.631210\n",
      "Train Epoch: 1 [488/526 (93%)]\tLoss: 37.571762\tTotal Loss: 23.659770\n",
      "Train Epoch: 1 [489/526 (93%)]\tLoss: 34.462399\tTotal Loss: 23.681860\n",
      "Train Epoch: 1 [490/526 (93%)]\tLoss: 23.790308\tTotal Loss: 23.682080\n",
      "Train Epoch: 1 [491/526 (93%)]\tLoss: 28.471287\tTotal Loss: 23.691840\n",
      "Train Epoch: 1 [492/526 (93%)]\tLoss: 16.620472\tTotal Loss: 23.677470\n",
      "Train Epoch: 1 [493/526 (94%)]\tLoss: 25.851994\tTotal Loss: 23.681880\n",
      "Train Epoch: 1 [494/526 (94%)]\tLoss: 47.861343\tTotal Loss: 23.730820\n",
      "Train Epoch: 1 [495/526 (94%)]\tLoss: 33.080570\tTotal Loss: 23.749710\n",
      "Train Epoch: 1 [496/526 (94%)]\tLoss: 28.929941\tTotal Loss: 23.760160\n",
      "Train Epoch: 1 [497/526 (94%)]\tLoss: 42.450157\tTotal Loss: 23.797760\n",
      "Train Epoch: 1 [498/526 (94%)]\tLoss: 7.221110\tTotal Loss: 23.764470\n",
      "Train Epoch: 1 [499/526 (95%)]\tLoss: 0.000000\tTotal Loss: 23.716850\n",
      "Train Epoch: 1 [500/526 (95%)]\tLoss: 5.873465\tTotal Loss: 23.681160\n",
      "Train Epoch: 1 [501/526 (95%)]\tLoss: 16.003277\tTotal Loss: 23.665840\n",
      "Train Epoch: 1 [502/526 (95%)]\tLoss: 39.144905\tTotal Loss: 23.696670\n",
      "Train Epoch: 1 [503/526 (95%)]\tLoss: 37.795517\tTotal Loss: 23.724700\n",
      "Train Epoch: 1 [504/526 (96%)]\tLoss: 36.301182\tTotal Loss: 23.749660\n",
      "Train Epoch: 1 [505/526 (96%)]\tLoss: 8.668364\tTotal Loss: 23.719790\n",
      "Train Epoch: 1 [506/526 (96%)]\tLoss: 46.563625\tTotal Loss: 23.764940\n",
      "Train Epoch: 1 [507/526 (96%)]\tLoss: 30.673517\tTotal Loss: 23.778560\n",
      "Train Epoch: 1 [508/526 (96%)]\tLoss: 0.120148\tTotal Loss: 23.731990\n",
      "Train Epoch: 1 [509/526 (97%)]\tLoss: 0.001074\tTotal Loss: 23.685370\n",
      "Train Epoch: 1 [510/526 (97%)]\tLoss: 27.161392\tTotal Loss: 23.692190\n",
      "Train Epoch: 1 [511/526 (97%)]\tLoss: 42.498096\tTotal Loss: 23.728990\n",
      "Train Epoch: 1 [512/526 (97%)]\tLoss: 7.668270\tTotal Loss: 23.697620\n",
      "Train Epoch: 1 [513/526 (97%)]\tLoss: 41.431740\tTotal Loss: 23.732190\n",
      "Train Epoch: 1 [514/526 (98%)]\tLoss: 32.278740\tTotal Loss: 23.748820\n",
      "Train Epoch: 1 [515/526 (98%)]\tLoss: 44.068878\tTotal Loss: 23.788270\n",
      "Train Epoch: 1 [516/526 (98%)]\tLoss: 34.767498\tTotal Loss: 23.809550\n",
      "Train Epoch: 1 [517/526 (98%)]\tLoss: 66.337883\tTotal Loss: 23.891810\n",
      "Train Epoch: 1 [518/526 (98%)]\tLoss: 44.930943\tTotal Loss: 23.932430\n",
      "Train Epoch: 1 [519/526 (98%)]\tLoss: 53.839264\tTotal Loss: 23.990050\n",
      "Train Epoch: 1 [520/526 (99%)]\tLoss: 0.000000\tTotal Loss: 23.943920\n",
      "Train Epoch: 1 [521/526 (99%)]\tLoss: 54.821548\tTotal Loss: 24.003180\n",
      "Train Epoch: 1 [522/526 (99%)]\tLoss: 25.858343\tTotal Loss: 24.006740\n",
      "Train Epoch: 1 [523/526 (99%)]\tLoss: 37.165562\tTotal Loss: 24.031900\n",
      "Train Epoch: 1 [524/526 (99%)]\tLoss: 40.429939\tTotal Loss: 24.063190\n",
      "Train Epoch: 1 [525/526 (100%)]\tLoss: 23.399345\tTotal Loss: 24.061930\n",
      "Train Epoch: 1 [526/526 (100%)]\tLoss: 9.835071\tTotal Loss: 24.034880\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nepochs):\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for batch_index, row in enumerate(train_data):\n",
    "        X = row['context']\n",
    "        y = []\n",
    "\n",
    "        for word in row['word']:\n",
    "            try:\n",
    "                y.append(w2id[word])\n",
    "            except:\n",
    "                y.append(w2id['oov'])\n",
    "\n",
    "        y = torch.tensor(y).to(device)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(X)\n",
    "\n",
    "        # Loss\n",
    "        loss = criterion(outputs, y)\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Display\n",
    "        if(batch_index + 1) % batch_status == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTotal Loss: {:.6f}'.format(\n",
    "                epoch + 1, batch_index + 1, len(train_data),\n",
    "                100. * batch_index / len(train_data), float(loss), \n",
    "                round(sum(losses) / len(losses), 5))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.lookup.weight.data.cpu().numpy()\n",
    "\n",
    "w2emb = { w: emb for (w, emb) in zip(vocab, list(embeddings)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('assassin', 1.0)\n",
      "('chases', 0.15653107)\n",
      "('ascends', 0.1514222)\n",
      "('mother', 0.13588268)\n",
      "('assassination', 0.12911765)\n",
      "('vice-president', 0.12196292)\n",
      "('center', 0.12073536)\n",
      "('railing', 0.11702977)\n",
      "('young', 0.114941105)\n",
      "('or', 0.114193514)\n"
     ]
    }
   ],
   "source": [
    "lookup_word = 'assassin'\n",
    "similarities = cosine_similarity([w2emb[lookup_word]], embeddings)[0]\n",
    "\n",
    "candidates = sorted([(vocab[i], sim) for i, sim in enumerate(similarities)], \n",
    "    key = lambda x: x[1], reverse = True)[:10]\n",
    "\n",
    "for candidate in candidates:\n",
    "    print(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram\n",
    "Arquitetura proposta junto com a CBOW. Contudo, a skip-gram aprende representações vetoriais ao ser treinada prevendo os tokens ao redor de um token alvo.\n",
    "\n",
    "Como já definimos o vocabulário anteriormente, vamos criar os exemplos para o treinamento da arquitetura. Note que agora daremos o token alvo como entrada e orientaremos a rede neural a prever as palavras ao redor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2647ea34e536f865ab67ff9ddee7fd78773d956cec0cab53c79b32cd10da5d83"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
